\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vasilescu2002multilinear}
\citation{cichocki2013tensor}
\citation{austin2016parallel}
\citation{sun2008incremental}
\citation{kolda2008scalable}
\citation{kolda2009tensor}
\citation{muthukrishnan2005data}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{malik2018low}
\citation{kolda2009tensor}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Related Work}{3}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Notation}{3}{subsection.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Tail energy}{3}{subsubsection.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Kronecker and Khatri-Rao product}{3}{subsubsection.9}}
\newlabel{kronecker}{{2.1}{3}{Kronecker and Khatri-Rao product}{equation.10}{}}
\newlabel{kronecker@cref}{{[equation][1][2]2.1}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Tensor basics}{3}{subsubsection.11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Tensor unfoldings}{3}{subsubsection.12}}
\newlabel{eq:F_norm_equivalent}{{2.2}{3}{Tensor unfoldings}{equation.13}{}}
\newlabel{eq:F_norm_equivalent@cref}{{[equation][2][2]2.2}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Tensor rank}{3}{subsubsection.14}}
\citation{kolda2009tensor}
\citation{de2000multilinear}
\citation{tucker1966some}
\citation{de2000multilinear}
\citation{tucker1966some}
\citation{de2000multilinear}
\citation{tucker1966some}
\citation{zhou2014decomposition}
\citation{battaglino2019faster}
\citation{halko2011finding}
\citation{vannieuwenhoven2012new}
\citation{vannieuwenhoven2012new}
\citation{de2000multilinear}
\citation{de2000multilinear}
\citation{de2000multilinear}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.6}Tensor contractions}{4}{subsubsection.15}}
\newlabel{eq: tensor_product_association}{{2.1.6}{4}{Tensor contractions}{subsubsection.15}{}}
\newlabel{eq: tensor_product_association@cref}{{[subsubsection][6][2,1]2.1.6}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Tucker Approximation}{4}{subsection.16}}
\newlabel{eq:tucker_optimization}{{2.3}{4}{Tucker Approximation}{equation.18}{}}
\newlabel{eq:tucker_optimization@cref}{{[equation][3][2]2.3}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}HOSVD}{4}{subsubsection.19}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:hosvd}{{2.1}{4}{Higher order singular value decomposition (HOSVD) \cite {de2000multilinear,tucker1966some} \relax }{algorithm.20}{}}
\newlabel{alg:hosvd@cref}{{[algorithm][1][2]2.1}{[1][4][]4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.1}{\ignorespaces Higher order singular value decomposition (HOSVD) \cite  {de2000multilinear,tucker1966some} \relax }}{4}{algorithm.20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}ST-HOSVD}{4}{subsubsection.23}}
\citation{malik2018low}
\citation{malik2018low}
\citation{malik2018low}
\citation{malik2018low}
\citation{halko2011finding}
\citation{woodruff2014sketching}
\citation{tropp2018more}
\citation{tropp2019streaming}
\citation{wang2015fast}
\citation{battaglino2018practical}
\citation{tsourakakis2010mach}
\citation{baskaran2012efficient}
\citation{zhou2014decomposition}
\citation{austin2016parallel}
\citation{kaya2016high}
\citation{li2015input}
\citation{battaglino2019faster}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}HOOI}{5}{subsubsection.24}}
\newlabel{alg:hooi}{{2.2}{5}{Higher order orthogonal iteration (HOOI) \cite {de2000multilinear} \relax }{algorithm.25}{}}
\newlabel{alg:hooi@cref}{{[algorithm][2][2]2.2}{[1][4][]5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.2}{\ignorespaces Higher order orthogonal iteration (HOOI) \cite  {de2000multilinear} \relax }}{5}{algorithm.25}}
\newlabel{eq:factor-update}{{2.4}{5}{Higher order orthogonal iteration (HOOI) \cite {de2000multilinear} \relax }{equation.27}{}}
\newlabel{eq:factor-update@cref}{{[equation][4][2]2.4}{[1][4][]5}}
\newlabel{eq:core_update}{{2.5}{5}{Higher order orthogonal iteration (HOOI) \cite {de2000multilinear} \relax }{equation.29}{}}
\newlabel{eq:core_update@cref}{{[equation][5][2]2.5}{[1][4][]5}}
\newlabel{sec: previous_work}{{2.3}{5}{Previous Work}{subsection.30}{}}
\newlabel{sec: previous_work@cref}{{[subsection][3][2]2.3}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Previous Work}{5}{subsection.30}}
\citation{oymak2015universality}
\citation{woolfe2008fast}
\citation{achlioptas2003database}
\citation{li2006very}
\citation{sun2018tensor}
\citation{sun2018tensor}
\citation{rudelson2012row}
\@writefile{toc}{\contentsline {section}{\numberline {3}Dimension Reduction Maps}{6}{section.32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dimension Reduction Map}{6}{subsection.33}}
\newlabel{s-trp}{{3.2}{6}{Tensor Random Projection}{subsection.34}{}}
\newlabel{s-trp@cref}{{[subsection][2][3]3.2}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Tensor Random Projection}{6}{subsection.34}}
\newlabel{eq:TRP}{{3.1}{6}{Tensor Random Projection}{equation.35}{}}
\newlabel{eq:TRP@cref}{{[equation][1][3]3.1}{[1][6][]6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance of Different Dimension Reduction Maps: We compare the storage cost and the computational cost of applying a DRM mapping $\mathbb  {R}^{I^N}$ to $\mathbb  {R}^k$ to a dense tensor in $\mathbb  {R}^{I^N}$. Here $\mu $ is the sparse factor for sparse random projection. The TRP considered here is composed of Gaussian DRMs. \relax }}{6}{table.36}}
\newlabel{tbl: random_map}{{1}{6}{Performance of Different Dimension Reduction Maps: We compare the storage cost and the computational cost of applying a DRM mapping $\mathbb {R}^{I^N}$ to $\mathbb {R}^k$ to a dense tensor in $\mathbb {R}^{I^N}$. Here $\mu $ is the sparse factor for sparse random projection. The TRP considered here is composed of Gaussian DRMs. \relax }{table.36}{}}
\newlabel{tbl: random_map@cref}{{[table][1][]1}{[1][6][]6}}
\citation{tropp2018more}
\citation{arora2009computational}
\@writefile{toc}{\contentsline {section}{\numberline {4}Algorithms for Tucker approximation}{7}{section.37}}
\newlabel{sec:sketch}{{4.1}{7}{Tensor compression via sketching}{subsection.38}{}}
\newlabel{sec:sketch@cref}{{[subsection][1][4]4.1}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Tensor compression via sketching}{7}{subsection.38}}
\@writefile{toc}{\contentsline {paragraph}{The Tucker sketch}{7}{section*.39}}
\newlabel{sketches}{{4.1}{7}{The Tucker sketch}{equation.40}{}}
\newlabel{sketches@cref}{{[equation][1][4]4.1}{[1][7][]7}}
\newlabel{eq:sketchy_matrix}{{4.1}{7}{The Tucker sketch}{equation.40}{}}
\newlabel{eq:sketchy_matrix@cref}{{[subsection][1][4]4.1}{[1][7][]7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.1}{\ignorespaces Tucker Sketch\relax }}{7}{algorithm.41}}
\newlabel{alg:tensor_sketch}{{4.1}{7}{Tucker Sketch\relax }{algorithm.41}{}}
\newlabel{alg:tensor_sketch@cref}{{[algorithm][1][4]4.1}{[1][7][]7}}
\@writefile{thm}{\contentsline {remark}{{Remark}{4.1}{}}{7}{theorem.42}}
\@writefile{thm}{\contentsline {remark}{{Remark}{4.2}{}}{8}{theorem.44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Low-Rank Approximation}{8}{subsection.45}}
\newlabel{eqn:qr}{{4.2}{8}{Low-Rank Approximation}{equation.46}{}}
\newlabel{eqn:qr@cref}{{[equation][2][4]4.2}{[1][8][]8}}
\newlabel{eq:x_tilde}{{4.3}{8}{Low-Rank Approximation}{equation.47}{}}
\newlabel{eq:x_tilde@cref}{{[equation][3][4]4.3}{[1][8][]8}}
\newlabel{eq: two-pass}{{4.4}{8}{Low-Rank Approximation}{equation.48}{}}
\newlabel{eq: two-pass@cref}{{[equation][4][4]4.4}{[1][8][]8}}
\newlabel{alg:two_pass_low_rank_appro}{{4.2}{8}{Two Pass Sketch and Low Rank Recovery \relax }{algorithm.49}{}}
\newlabel{alg:two_pass_low_rank_appro@cref}{{[algorithm][2][4]4.2}{[1][8][]8}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.2}{\ignorespaces Two Pass Sketch and Low Rank Recovery \relax }}{8}{algorithm.49}}
\@writefile{toc}{\contentsline {paragraph}{One-Pass Approximation}{8}{section*.53}}
\citation{malik2018low}
\citation{tropp2019streaming}
\citation{vannieuwenhoven2012new}
\citation{ballester2019tthresh}
\newlabel{alg:one_pass_low_rank_appro}{{4.3}{9}{One Pass Sketch and Low Rank Recovery \relax }{algorithm.54}{}}
\newlabel{alg:one_pass_low_rank_appro@cref}{{[algorithm][3][4]4.3}{[1][9][]9}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.3}{\ignorespaces One Pass Sketch and Low Rank Recovery \relax }}{9}{algorithm.54}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Computational Complexity of \ref  {alg:one_pass_low_rank_appro} on tensor $\T {X} \in \mathbb  {R}^{I \times \dots  \times I}$ with parameters $(k,s)$, using a TRP composed of Gaussian DRMs inside the Tucker sketch. By far the majority of the time is spent sketching the tensor $\T {X}$. \relax }}{9}{table.58}}
\newlabel{tbl: time-complexity}{{2}{9}{Computational Complexity of \ref {alg:one_pass_low_rank_appro} on tensor $\T {X} \in \mathbb {R}^{I \times \dots \times I}$ with parameters $(k,s)$, using a TRP composed of Gaussian DRMs inside the Tucker sketch. By far the majority of the time is spent sketching the tensor $\T {X}$. \relax }{table.58}{}}
\newlabel{tbl: time-complexity@cref}{{[table][2][]2}{[1][9][]9}}
\newlabel{sec:fixed_rank}{{4.3}{9}{Fixed-Rank Approximation}{subsection.59}{}}
\newlabel{sec:fixed_rank@cref}{{[subsection][3][4]4.3}{[1][9][]9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Fixed-Rank Approximation}{9}{subsection.59}}
\@writefile{thm}{\contentsline {lem}{{Lemma}{4.{1}}{}}{9}{lem.60}}
\newlabel{lemma: equivalance_one_pass}{{4.{1}}{9}{Fixed-Rank Approximation}{lem.60}{}}
\newlabel{lemma: equivalance_one_pass@cref}{{[lem][1][4]4.{1}}{[1][9][]9}}
\newlabel{alg:one_pass_fix_rank_appro}{{4.4}{10}{Fixed rank approximation \relax }{algorithm.62}{}}
\newlabel{alg:one_pass_fix_rank_appro@cref}{{[algorithm][4][4]4.4}{[1][10][]10}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.4}{\ignorespaces Fixed rank approximation \relax }}{10}{algorithm.62}}
\newlabel{line:core-decom}{{1}{10}{Fixed rank approximation \relax }{Item.63}{}}
\newlabel{line:core-decom@cref}{{[enumi][1][]1}{[1][10][]10}}
\newlabel{sec:theory}{{5}{10}{Guarantees}{section.65}{}}
\newlabel{sec:theory@cref}{{[section][5][]5}{[1][10][]10}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Guarantees}{10}{section.65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Low rank approximation}{10}{subsection.66}}
\@writefile{thm}{\contentsline {thm}{{Theorem}{5.{1}}{}}{10}{thm.67}}
\newlabel{thm:low_rank_err_two_pass}{{5.{1}}{10}{Low rank approximation}{thm.67}{}}
\newlabel{thm:low_rank_err_two_pass@cref}{{[thm][1][5]5.{1}}{[1][10][]10}}
\@writefile{thm}{\contentsline {thm}{{Theorem}{5.{2}}{}}{10}{thm.68}}
\newlabel{thm:low_rank_err}{{5.{2}}{10}{Low rank approximation}{thm.68}{}}
\newlabel{thm:low_rank_err@cref}{{[thm][2][5]5.{2}}{[1][10][]10}}
\@writefile{thm}{\contentsline {remark}{{Remark}{5.1}{}}{11}{theorem.69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Fixed rank approximation}{11}{subsection.71}}
\@writefile{thm}{\contentsline {thm}{{Theorem}{5.{3}}{}}{11}{thm.72}}
\newlabel{thm:fix_rank_err}{{5.{3}}{11}{Fixed rank approximation}{thm.72}{}}
\newlabel{thm:fix_rank_err@cref}{{[thm][3][5]5.{3}}{[1][11][]11}}
\newlabel{eq-fixed-rank-bound}{{5.{3}}{11}{Fixed rank approximation}{thm.72}{}}
\newlabel{eq-fixed-rank-bound@cref}{{[thm][3][5]5.{3}}{[1][11][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Proof sketch}{11}{subsection.73}}
\citation{malik2018low}
\citation{malik2018low}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textit  {Different DRMs perform similarly.} We approximate 3D synthetic tensors (see \ref  {s-synthetic-data}) with $I = 600$, using our one-pass algorithm with $r = 5$ and varying $k$ ($s = 2k+1$), using a variety of DRMs in the Tucker sketch: Gaussian, SSRFT, Gaussian TRP, or Sparse TRP.  \relax }}{12}{figure.75}}
\newlabel{fig:vary-k-600}{{1}{12}{\textit {Different DRMs perform similarly.} We approximate 3D synthetic tensors (see \ref {s-synthetic-data}) with $I = 600$, using our one-pass algorithm with $r = 5$ and varying $k$ ($s = 2k+1$), using a variety of DRMs in the Tucker sketch: Gaussian, SSRFT, Gaussian TRP, or Sparse TRP.  \relax }{figure.75}{}}
\newlabel{fig:vary-k-600@cref}{{[figure][1][]1}{[1][12][]12}}
\newlabel{s-experiments}{{6}{12}{Numerical Experiments}{section.74}{}}
\newlabel{s-experiments@cref}{{[section][6][]6}{[1][12][]12}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical Experiments}{12}{figure.76}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textit  {Two-pass improves on one-pass.} We approximate 3D synthetic tensors (see \ref  {s-synthetic-data}) with $I = 600$, using our one-pass and two-pass algorithms with $r = 5$ and varying $k$ ($s = 2k+1$), using the Gaussian TRP in the Tucker sketch.  \relax }}{13}{figure.76}}
\newlabel{fig:vary-k-600-compare}{{2}{13}{\textit {Two-pass improves on one-pass.} We approximate 3D synthetic tensors (see \ref {s-synthetic-data}) with $I = 600$, using our one-pass and two-pass algorithms with $r = 5$ and varying $k$ ($s = 2k+1$), using the Gaussian TRP in the Tucker sketch.  \relax }{figure.76}{}}
\newlabel{fig:vary-k-600-compare@cref}{{[figure][2][]2}{[1][12][]13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  \textit  {Faster approximations.} We approximate 3D synthetic tensors with $I = 600$ generated as described in \ref  {s-synthetic-data}, using HOOI and our one-pass and two-pass algorithms with $r = 5$ for a few different $k$ ($s = 2k+1$). \relax }}{13}{figure.77}}
\newlabel{fig:run_time}{{3}{13}{\textit {Faster approximations.} We approximate 3D synthetic tensors with $I = 600$ generated as described in \ref {s-synthetic-data}, using HOOI and our one-pass and two-pass algorithms with $r = 5$ for a few different $k$ ($s = 2k+1$). \relax }{figure.77}{}}
\newlabel{fig:run_time@cref}{{[figure][3][]3}{[1][12][]13}}
\newlabel{s-synthetic-data}{{6.1}{13}{Synthetic experiments}{subsection.78}{}}
\newlabel{s-synthetic-data@cref}{{[subsection][1][6]6.1}{[1][12][]13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Synthetic experiments}{13}{subsection.78}}
\citation{malik2018low}
\citation{malik2018low}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textit  {Approximations improve with more memory: synthetic data.} We approximate 3D synthetic tensors (see \ref  {s-synthetic-data}) with $I = 300$, using T.-TS and our one-pass and two-pass algorithms with the Gaussian TRP to produce approximations with equal ranks $r=10$. Notice every marker on the plot corresponds to a 2700$\times $ compression!\relax }}{14}{figure.80}}
\newlabel{fig:vary-memory}{{4}{14}{\textit {Approximations improve with more memory: synthetic data.} We approximate 3D synthetic tensors (see \ref {s-synthetic-data}) with $I = 300$, using T.-TS and our one-pass and two-pass algorithms with the Gaussian TRP to produce approximations with equal ranks $r=10$. Notice every marker on the plot corresponds to a 2700$\times $ compression!\relax }{figure.80}{}}
\newlabel{fig:vary-memory@cref}{{[figure][4][]4}{[1][14][]14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Different dimension reduction maps perform similarly}{14}{subsubsection.79}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}A second pass reduces error}{14}{subsubsection.85}}
\citation{malik2018low}
\citation{malik2018low}
\citation{hurrell2013community}
\citation{kay2015community}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textit  {Approximations improves with more memory: real data.} We approximate aerosol absorption and combustion data using our one-pass and two-pass algorithms with the Gaussian TRP. We compare three target ranks ($r/I = 0.125,0.1,0.067$) for the former, and use the same target rank ($r/I = 0.1$) for each measured quantity in the combustion dataset. Notice $r/I = 0.1$ gives a hundred-fold compression! \relax }}{15}{figure.81}}
\newlabel{fig:climate}{{5}{15}{\textit {Approximations improves with more memory: real data.} We approximate aerosol absorption and combustion data using our one-pass and two-pass algorithms with the Gaussian TRP. We compare three target ranks ($r/I = 0.125,0.1,0.067$) for the former, and use the same target rank ($r/I = 0.1$) for each measured quantity in the combustion dataset. Notice $r/I = 0.1$ gives a hundred-fold compression! \relax }{figure.81}{}}
\newlabel{fig:climate@cref}{{[figure][5][]5}{[1][14][]15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Improvement on state-of-the-art}{15}{subsubsection.86}}
\@writefile{thm}{\contentsline {remark}{{Remark}{6.1}{}}{15}{theorem.87}}
\newlabel{s-real-data}{{6.2}{15}{Applications}{subsection.88}{}}
\newlabel{s-real-data@cref}{{[subsection][2][6]6.2}{[1][15][]15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Applications}{15}{subsection.88}}
\citation{lapointe2015differential}
\citation{malik2018low}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textit  {Video Scene Classification} ($2200 \times 1080 \times 1980$): We classify frames from the video data from \cite  {malik2018low} (collected as a third order tensor with size $2200 \times 1080 \times 1980$) using $K$-means with $K$=3 on vectors computed using four different methods. $s = 2k+1$ throughout. 1) The linear sketch along the time dimension (Row 1). 2-3) the Tucker factor along the time dimension, computed via our two-pass (Row 2) and one-pass (Row 3) algorithms. 4) The Tucker factor along the time dimension, computed via our one-pass (Row 4) algorithm \relax }}{16}{figure.82}}
\newlabel{fig:video}{{6}{16}{\textit {Video Scene Classification} ($2200 \times 1080 \times 1980$): We classify frames from the video data from \cite {malik2018low} (collected as a third order tensor with size $2200 \times 1080 \times 1980$) using $K$-means with $K$=3 on vectors computed using four different methods. $s = 2k+1$ throughout. 1) The linear sketch along the time dimension (Row 1). 2-3) the Tucker factor along the time dimension, computed via our two-pass (Row 2) and one-pass (Row 3) algorithms. 4) The Tucker factor along the time dimension, computed via our one-pass (Row 4) algorithm \relax }{figure.82}{}}
\newlabel{fig:video@cref}{{[figure][6][]6}{[1][14][]16}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textit  {Visualizing Video Recovery:} Original frame (left); approximation by two-pass sketch (middle); approximation by one-pass sketch (right). \relax }}{16}{figure.83}}
\newlabel{fig:Frame500}{{7}{16}{\textit {Visualizing Video Recovery:} Original frame (left); approximation by two-pass sketch (middle); approximation by one-pass sketch (right). \relax }{figure.83}{}}
\newlabel{fig:Frame500@cref}{{[figure][7][]7}{[1][14][]16}}
\citation{malik2018low}
\citation{malik2018low}
\citation{malik2018low}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textit  {Visualizing Combustion Simulation:} All four figures show a slice of the temperature data along the first dimension. The approximation uses $\mathbf  {r} = (281,25,25)$, $\mathbf  {k} = (562,50,50)$, $\mathbf  {s} = (1125, 101, 101)$, with the Gaussian TRP in the Tucker sketch.\relax }}{17}{figure.84}}
\newlabel{fig:T100}{{8}{17}{\textit {Visualizing Combustion Simulation:} All four figures show a slice of the temperature data along the first dimension. The approximation uses $\mathbf {r} = (281,25,25)$, $\mathbf {k} = (562,50,50)$, $\mathbf {s} = (1125, 101, 101)$, with the Gaussian TRP in the Tucker sketch.\relax }{figure.84}{}}
\newlabel{fig:T100@cref}{{[figure][8][]8}{[1][14][]17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Data compression}{17}{subsubsection.89}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Video scene classification}{17}{subsubsection.90}}
\bibstyle{siamplain}
\bibdata{bibtex}
\bibcite{achlioptas2003database}{1}
\bibcite{ailon2009fast}{2}
\bibcite{arora2009computational}{3}
\bibcite{austin2016parallel}{4}
\bibcite{ballester2019tthresh}{5}
\bibcite{baskaran2012efficient}{6}
\bibcite{battaglino2018practical}{7}
\bibcite{battaglino2019faster}{8}
\bibcite{boutsidis2013improved}{9}
\bibcite{cichocki2013tensor}{10}
\bibcite{clarkson2017low}{11}
\bibcite{cormode2008finding}{12}
\bibcite{de2000multilinear}{13}
\bibcite{de2008tensor}{14}
\bibcite{2017arXiv171209473D}{15}
\bibcite{halko2011finding}{16}
\bibcite{hurrell2013community}{17}
\bibcite{kay2015community}{18}
\bibcite{kaya2016high}{19}
\bibcite{kolda2009tensor}{20}
\bibcite{kolda2008scalable}{21}
\bibcite{lapointe2015differential}{22}
\bibcite{li2015input}{23}
\@writefile{toc}{\contentsline {section}{References}{19}{section*.91}}
\bibcite{li2006very}{24}
\bibcite{malik2018low}{25}
\bibcite{muthukrishnan2005data}{26}
\bibcite{oymak2015universality}{27}
\bibcite{rudelson2012row}{28}
\bibcite{sun2008incremental}{29}
\bibcite{sun2018tensor}{30}
\bibcite{tropp2011improved}{31}
\bibcite{tropp2017practical}{32}
\bibcite{tropp2018more}{33}
\bibcite{tropp2019streaming}{34}
\bibcite{tsourakakis2010mach}{35}
\bibcite{tucker1966some}{36}
\bibcite{vannieuwenhoven2012new}{37}
\bibcite{vasilescu2002multilinear}{38}
\bibcite{wang2015fast}{39}
\bibcite{woodruff2014sketching}{40}
\bibcite{woolfe2008fast}{41}
\bibcite{zhou2014decomposition}{42}
\citation{de2008tensor}
\citation{vannieuwenhoven2012new}
\@writefile{toc}{\contentsline {section}{Appendix Appendix\nobreakspace  {}A. Proof of Main Results}{21}{Appendix.92}}
\newlabel{appendix:proof-main-result}{{A}{21}{Acknowledgments}{Appendix.92}{}}
\newlabel{appendix:proof-main-result@cref}{{[appendix][1][2147483647]A}{[1][21][]21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Error bound for the two pass approximation Algorithm \ref  {alg:two_pass_low_rank_appro}}{21}{proof.94}}
\@writefile{thm}{\contentsline {proof}{{Proof}{1}{Proof of Theorem \ref {thm:low_rank_err_two_pass}}}{21}{proof.94}}
\global\def\markiproofii{\proofbox }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Error bound for the one pass approximation Algorithm \ref  {alg:one_pass_low_rank_appro}}{21}{proof.97}}
\@writefile{thm}{\contentsline {proof}{{Proof}{2}{Proof of Theorem \ref {thm:low_rank_err}}}{21}{proof.97}}
\newlabel{eq:inner_zero}{{A.3}{21}{Error bound for the one pass approximation Algorithm \ref {alg:one_pass_low_rank_appro}}{equation.100}{}}
\newlabel{eq:inner_zero@cref}{{[equation][3][2147483647,1]A.3}{[1][21][]21}}
\newlabel{eq:error_decom}{{A.4}{21}{Error bound for the one pass approximation Algorithm \ref {alg:one_pass_low_rank_appro}}{equation.101}{}}
\newlabel{eq:error_decom@cref}{{[equation][4][2147483647,1]A.4}{[1][21][]21}}
\citation{tropp2017practical}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Error bound for the fixed rank approximation Algorithm \ref  {alg:one_pass_fix_rank_appro}}{22}{proof.105}}
\@writefile{thm}{\contentsline {proof}{{Proof}{3}{Proof of Theorem \ref {thm:fix_rank_err}}}{22}{proof.105}}
\@writefile{toc}{\contentsline {section}{Appendix Appendix\nobreakspace  {}B. Probabilistic Analysis of Core Sketch Error}{22}{Appendix.107}}
\newlabel{eq: def-proj-Q}{{B.1}{22}{Error bound for the fixed rank approximation Algorithm \ref {alg:one_pass_fix_rank_appro}}{equation.108}{}}
\newlabel{eq: def-proj-Q@cref}{{[equation][1][2147483647,2]B.1}{[1][22][]22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Decomposition of Core Approximation Error}{22}{subsection.109}}
\@writefile{thm}{\contentsline {lem}{{Lemma}{B.{1}}{}}{22}{lem.110}}
\newlabel{lemma:core_error_decomposition}{{B.{1}}{22}{Decomposition of Core Approximation Error}{lem.110}{}}
\newlabel{lemma:core_error_decomposition@cref}{{[lem][1][2147483647,2]B.{1}}{[1][22][]22}}
\newlabel{eq:def_each_part}{{B.2}{22}{Decomposition of Core Approximation Error}{equation.112}{}}
\newlabel{eq:def_each_part@cref}{{[equation][2][2147483647,2]B.2}{[1][22][]22}}
\@writefile{thm}{\contentsline {proof}{{Proof}{4}{}}{23}{proof.113}}
\newlabel{eq:core_err_decom}{{B.3}{23}{Decomposition of Core Approximation Error}{equation.117}{}}
\newlabel{eq:core_err_decom@cref}{{[equation][3][2147483647,2]B.3}{[1][23][]23}}
\global\def\markivproofvi{\proofbox }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Probabilistic Core Error Bound}{23}{subsection.121}}
\@writefile{thm}{\contentsline {lem}{{Lemma}{B.{2}}{}}{23}{lem.122}}
\newlabel{lemma:err_core_sketch}{{B.{2}}{23}{Probabilistic Core Error Bound}{lem.122}{}}
\newlabel{lemma:err_core_sketch@cref}{{[lem][2][2147483647,2]B.{2}}{[1][23][]23}}
\@writefile{thm}{\contentsline {proof}{{Proof}{5}{}}{23}{proof.124}}
\newlabel{eq:factor-matrix-error-bounds-core-error}{{B.4}{23}{Probabilistic Core Error Bound}{equation.125}{}}
\newlabel{eq:factor-matrix-error-bounds-core-error@cref}{{[equation][4][2147483647,2]B.4}{[1][23][]23}}
\newlabel{eq:inner_prod2}{{B.5}{24}{Probabilistic Core Error Bound}{equation.127}{}}
\newlabel{eq:inner_prod2@cref}{{[equation][5][2147483647,2]B.5}{[1][24][]24}}
\global\def\markvproofvi{\proofbox }
\@writefile{toc}{\contentsline {section}{Appendix Appendix\nobreakspace  {}C. Proof of fixed rank approximation lemma}{24}{Appendix.129}}
\newlabel{appendix: proof-fix-rank-lemma}{{C}{24}{Probabilistic Core Error Bound}{Appendix.129}{}}
\newlabel{appendix: proof-fix-rank-lemma@cref}{{[appendix][3][2147483647]C}{[1][24][]24}}
\@writefile{thm}{\contentsline {proof}{{Proof}{6}{Proof of \ref {lemma: equivalance_one_pass}}}{24}{proof.130}}
\citation{halko2011finding}
\@writefile{toc}{\contentsline {section}{Appendix Appendix\nobreakspace  {}D. Technical Lemmas}{25}{Appendix.133}}
\newlabel{s-matrix-projections}{{D.1}{25}{Random projections of matrices}{subsection.134}{}}
\newlabel{s-matrix-projections@cref}{{[subappendix][1][2147483647,4]D.1}{[1][25][]25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Random projections of matrices}{25}{subsection.134}}
\@writefile{thm}{\contentsline {lem}{{Lemma}{D.{1}}{}}{25}{lem.135}}
\newlabel{lemma:expectation_inverse_gaussian}{{D.{1}}{25}{Random projections of matrices}{lem.135}{}}
\newlabel{lemma:expectation_inverse_gaussian@cref}{{[lem][1][2147483647,4]D.{1}}{[1][25][]25}}
\@writefile{thm}{\contentsline {lem}{{Lemma}{D.{2}}{}}{25}{lem.137}}
\newlabel{lemma:sketchy_column_space_err}{{D.{2}}{25}{Random projections of matrices}{lem.137}{}}
\newlabel{lemma:sketchy_column_space_err@cref}{{[lem][2][2147483647,4]D.{2}}{[1][25][]25}}
\@writefile{toc}{\contentsline {section}{Appendix Appendix\nobreakspace  {}E. More Algorithms}{25}{Appendix.139}}
\newlabel{appendix:more_algorithms}{{E}{25}{Random projections of matrices}{Appendix.139}{}}
\newlabel{appendix:more_algorithms@cref}{{[appendix][5][2147483647]E}{[1][25][]25}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {E.1}{\ignorespaces Linear Update to Sketches\relax }}{25}{algorithm.140}}
\newlabel{alg:linear_update}{{E.1}{25}{Linear Update to Sketches\relax }{algorithm.140}{}}
\newlabel{alg:linear_update@cref}{{[algorithm][1][2147483647,5]E.1}{[1][25][]25}}
\@writefile{toc}{\contentsline {section}{Appendix Appendix\nobreakspace  {}F. Scrambled Subsampled Randomized Fourier Transform}{25}{Appendix.142}}
\newlabel{appendix: ssrft}{{F}{25}{Random projections of matrices}{Appendix.142}{}}
\newlabel{appendix: ssrft@cref}{{[appendix][6][2147483647]F}{[1][25][]25}}
\citation{boutsidis2013improved}
\citation{tropp2011improved}
\citation{ailon2009fast}
\citation{2017arXiv171209473D}
\citation{malik2018low}
\citation{malik2018low}
\citation{cormode2008finding}
\citation{clarkson2017low}
\@writefile{loa}{\contentsline {algorithm}{\numberline {E.2}{\ignorespaces Sketching in Distributed Setting\relax }}{26}{algorithm.141}}
\newlabel{alg:sketch_distributed}{{E.2}{26}{Sketching in Distributed Setting\relax }{algorithm.141}{}}
\newlabel{alg:sketch_distributed@cref}{{[algorithm][2][2147483647,5]E.2}{[1][25][]26}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {F.1}{\ignorespaces Scrambled Subsampled Randomized Fourier Transform (Row Linear Transform)\relax }}{26}{algorithm.144}}
\newlabel{alg:ssrft}{{F.1}{26}{Scrambled Subsampled Randomized Fourier Transform (Row Linear Transform)\relax }{algorithm.144}{}}
\newlabel{alg:ssrft@cref}{{[algorithm][1][2147483647,6]F.1}{[1][26][]26}}
\@writefile{toc}{\contentsline {section}{Appendix Appendix\nobreakspace  {}G. TensorSketch}{26}{Appendix.145}}
\newlabel{appendix: TensorSketch}{{G}{26}{Random projections of matrices}{Appendix.145}{}}
\newlabel{appendix: TensorSketch@cref}{{[appendix][7][2147483647]G}{[1][26][]26}}
\@writefile{toc}{\contentsline {paragraph}{CountSketch}{26}{section*.146}}
\citation{malik2018low}
\citation{malik2018low}
\citation{cormode2008finding}
\citation{malik2018low}
\citation{malik2018low}
\citation{malik2018low}
\citation{malik2018low}
\@writefile{toc}{\contentsline {paragraph}{TensorSketch}{27}{section*.149}}
\newlabel{eq: tucker-stage-1}{{G.1}{27}{TensorSketch}{equation.150}{}}
\newlabel{eq: tucker-stage-1@cref}{{[equation][1][2147483647,7]G.1}{[1][27][]27}}
\newlabel{eq: tucker-stage-2}{{G.2}{27}{TensorSketch}{equation.151}{}}
\newlabel{eq: tucker-stage-2@cref}{{[equation][2][2147483647,7]G.2}{[1][27][]27}}
\newlabel{eq:tensorsketch}{{G.3}{27}{TensorSketch}{equation.152}{}}
\newlabel{eq:tensorsketch@cref}{{[equation][3][2147483647,7]G.3}{[1][27][]27}}
\@writefile{toc}{\contentsline {section}{Appendix Appendix\nobreakspace  {}H. Time and Storage Complexity}{27}{Appendix.153}}
\newlabel{appendix: time-complexity}{{H}{27}{TensorSketch}{Appendix.153}{}}
\newlabel{appendix: time-complexity@cref}{{[appendix][8][2147483647]H}{[1][27][]27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {H.1}Comparison Between \cref  {alg:one_pass_fix_rank_appro} and T.-TS \cite  {malik2018low}}{27}{subsection.154}}
\citation{halko2011finding}
\@writefile{toc}{\contentsline {subsection}{\numberline {H.2}Computational Complexity of \cref  {alg:one_pass_fix_rank_appro}}{28}{subsection.155}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Storage complexity of \cref  {alg:one_pass_low_rank_appro} and T.-TS on tensor $\T {X} \in \mathbb  {R}^{I \times \dots  \times I}$. \cref  {alg:one_pass_low_rank_appro} uses parameters $(k,s) = (2r, 4r+1)$ and uses a TRP composed of Gaussian DRMs inside the Tucker sketch. T.-TS uses default values for hyper-parameters: $J_1=10r^{N-1}, J_2=10r^{N}$.\relax }}{28}{table.156}}
\newlabel{tab:storage-comparison}{{3}{28}{Storage complexity of \cref {alg:one_pass_low_rank_appro} and T.-TS on tensor $\T {X} \in \mathbb {R}^{I \times \dots \times I}$. \cref {alg:one_pass_low_rank_appro} uses parameters $(k,s) = (2r, 4r+1)$ and uses a TRP composed of Gaussian DRMs inside the Tucker sketch. T.-TS uses default values for hyper-parameters: $J_1=10r^{N-1}, J_2=10r^{N}$.\relax }{table.156}{}}
\newlabel{tab:storage-comparison@cref}{{[table][3][2147483647]3}{[1][28][]28}}
\@writefile{toc}{\contentsline {section}{Appendix Appendix\nobreakspace  {}I. More Numerics}{28}{Appendix.158}}
\newlabel{appendix:more_result}{{I}{28}{Computational Complexity of \cref {alg:one_pass_fix_rank_appro}}{Appendix.158}{}}
\newlabel{appendix:more_result@cref}{{[appendix][9][2147483647]I}{[1][28][]28}}
\newlabel{appendix:more_real_data_result}{{I}{28}{Computational Complexity of \cref {alg:one_pass_fix_rank_appro}}{figure.162}{}}
\newlabel{appendix:more_real_data_result@cref}{{[appendix][9][2147483647]I}{[1][28][]28}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Time complexity of \cref  {alg:one_pass_low_rank_appro} and T.-TS on tensor $\T {X} \in \mathbb  {R}^{I \times \dots  \times I}$. \cref  {alg:one_pass_low_rank_appro} uses parameters $(k,s) = (2r, 4r+1)$ and uses a TRP composed of Gaussian DRMs inside the Tucker sketch. T.-TS uses default values for hyper-parameters: $J_1=10r^{N-1}, J_2=10r^{N}$. \relax }}{29}{table.157}}
\newlabel{tab:time-comparison}{{4}{29}{Time complexity of \cref {alg:one_pass_low_rank_appro} and T.-TS on tensor $\T {X} \in \mathbb {R}^{I \times \dots \times I}$. \cref {alg:one_pass_low_rank_appro} uses parameters $(k,s) = (2r, 4r+1)$ and uses a TRP composed of Gaussian DRMs inside the Tucker sketch. T.-TS uses default values for hyper-parameters: $J_1=10r^{N-1}, J_2=10r^{N}$. \relax }{table.157}{}}
\newlabel{tab:time-comparison@cref}{{[table][4][2147483647]4}{[1][28][]29}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces We approximate 3D synthetic tensors (see \cref  {s-synthetic-data}) with $I = 400$, using our one-pass algorithm with $r = 5$ and varying $k$ ($s = 2k+1$), using a variety of DRMs in the Tucker sketch: Gaussian, SSRFT, Gaussian TRP, or Sparse TRP.\relax }}{29}{figure.159}}
\newlabel{fig:vary-k-400-app}{{9}{29}{We approximate 3D synthetic tensors (see \cref {s-synthetic-data}) with $I = 400$, using our one-pass algorithm with $r = 5$ and varying $k$ ($s = 2k+1$), using a variety of DRMs in the Tucker sketch: Gaussian, SSRFT, Gaussian TRP, or Sparse TRP.\relax }{figure.159}{}}
\newlabel{fig:vary-k-400-app@cref}{{[figure][9][2147483647]9}{[1][28][]29}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces We approximate 3D synthetic tensors (see \cref  {s-synthetic-data}) with $I = 200$, using our one-pass algorithm with $r = 5$ and varying $k$ ($s = 2k+1$), using a variety of DRMs in the Tucker sketch: Gaussian, SSRFT, Gaussian TRP, or Sparse TRP.\relax }}{30}{figure.160}}
\newlabel{fig:vary-k-200-app}{{10}{30}{We approximate 3D synthetic tensors (see \cref {s-synthetic-data}) with $I = 200$, using our one-pass algorithm with $r = 5$ and varying $k$ ($s = 2k+1$), using a variety of DRMs in the Tucker sketch: Gaussian, SSRFT, Gaussian TRP, or Sparse TRP.\relax }{figure.160}{}}
\newlabel{fig:vary-k-200-app@cref}{{[figure][10][2147483647]10}{[1][28][]30}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces We approximate 3D synthetic tensors (see \cref  {s-synthetic-data}) with $I = 400$, using our one-pass and two-pass algorithms with $r = 5$ and varying $k$ ($s = 2k+1$), using the Gaussian TRP in the Tucker sketch.\relax }}{30}{figure.161}}
\newlabel{fig:vary-k-400-compare-app}{{11}{30}{We approximate 3D synthetic tensors (see \cref {s-synthetic-data}) with $I = 400$, using our one-pass and two-pass algorithms with $r = 5$ and varying $k$ ($s = 2k+1$), using the Gaussian TRP in the Tucker sketch.\relax }{figure.161}{}}
\newlabel{fig:vary-k-400-compare-app@cref}{{[figure][11][2147483647]11}{[1][28][]30}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces We approximate 3D synthetic tensors (see \cref  {s-synthetic-data}) with $I = 200$, using our one-pass and two-pass algorithms with $r = 5$ and varying $k$ ($s = 2k+1$), using the Gaussian TRP in the Tucker sketch.\relax }}{31}{figure.162}}
\newlabel{fig:vary-k-200-compare-app}{{12}{31}{We approximate 3D synthetic tensors (see \cref {s-synthetic-data}) with $I = 200$, using our one-pass and two-pass algorithms with $r = 5$ and varying $k$ ($s = 2k+1$), using the Gaussian TRP in the Tucker sketch.\relax }{figure.162}{}}
\newlabel{fig:vary-k-200-compare-app@cref}{{[figure][12][2147483647]12}{[1][28][]31}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces We approximate the net radiative flux and dust aerosol burden data using our one-pass and two-pass algorithms using Gaussian TRP. We compare the performance under different ranks ($r/I = 0.125,0.2,0.067$). The dataset comes from the CESM CAM. The dust aerosol burden measures the amount of aerosol contributed by the dust. The net radiative flux determines the energy received by the earth surface through radiation. \relax }}{31}{figure.163}}
\newlabel{fig:srfrad_burden_dust}{{13}{31}{We approximate the net radiative flux and dust aerosol burden data using our one-pass and two-pass algorithms using Gaussian TRP. We compare the performance under different ranks ($r/I = 0.125,0.2,0.067$). The dataset comes from the CESM CAM. The dust aerosol burden measures the amount of aerosol contributed by the dust. The net radiative flux determines the energy received by the earth surface through radiation. \relax }{figure.163}{}}
\newlabel{fig:srfrad_burden_dust@cref}{{[figure][13][2147483647]13}{[1][28][]31}}
