\relax 
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{Dedication}{5}}
\@writefile{toc}{\contentsline {section}{Acknowledgements}{6}}
\newlabel{toc}{{}{8}}
\@writefile{toc}{\contentsline {section}{Table of Contents}{8}}
\citation{malik2018low}
\citation{granger1969investigating}
\citation{bowyer2016coherence}
\citation{bowyer2016coherence}
\citation{euan2016hierarchical}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Large Spectral Density Matrix Estimation by Thresholding}{2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{2}}
\newlabel{introduction}{{1.1}{2}}
\citation{brillinger1981time,brockwell2013time}
\citation{dahlhaus1997identification,dahlhaus2003causality,eichler2007frequency}
\citation{wu2015uniform}
\citation{bohm2009shrinkage}
\citation{bohm2008structural,fiecas2016dynamic,fiecas2014datadriven}
\citation{bickel2008covariance,rothman2009generalized,cai2011adaptive,cai2016rates}
\citation{ledoit2004well}
\citation{bickel2008covariance}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Background and Methods}{7}}
\newlabel{sec:model-methods}{{1.2}{7}}
\citation{Basu2015}
\citation{brockwell2013time,rosenblatt1985stationary}
\newlabel{def:coherance}{{1.2.2}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Background: Periodogram Smoothing and Shrinkage}{8}}
\newlabel{sec:model_method_background}{{1.2.1}{8}}
\newlabel{eq:single_periodogram}{{1.2.3}{8}}
\citation{brockwell2013time}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Threshold Selection by Frequency Domain Sample-splitting\relax }}{9}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:sample-split}{{1}{9}}
\newlabel{al1}{{1}{9}}
\newlabel{eq:cos_sin_coef}{{1.2.4}{9}}
\newlabel{eq:general_smoothing_estimator}{{1.2.5}{9}}
\citation{brockwell2013time}
\citation{brockwell2013time}
\citation{ledoit2004well}
\citation{bohm2009shrinkage}
\citation{bohm2009shrinkage}
\citation{ledoit2004well}
\newlabel{eq:smoothing estimator}{{1.2.6}{10}}
\citation{bohm2009shrinkage}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Method: Thresholding Averaged Periodogram}{11}}
\newlabel{sec:method_threshold}{{1.2.2}{11}}
\citation{rothman2009generalized}
\citation{rothman2009generalized}
\citation{rothman2009generalized}
\citation{jung2015learning,jung2015graphical}
\citation{brockwell2013time,bohm2009shrinkage}
\citation{ombao2001smoothing}
\citation{fiecas2014datadriven}
\citation{bickel2008covariance}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Choice of Tuning Parameters}{14}}
\newlabel{sec:tuning-parameter}{{1.2.3}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Theoretical Properties}{15}}
\newlabel{sec:theory}{{1.3}{15}}
\citation{van2016lecture}
\citation{bickel2008covariance}
\citation{cai2016rates}
\citation{cai2012minimax}
\citation{bickel2008covariance}
\citation{rudelson2013hanson}
\citation{Basu2015}
\newlabel{assumption:finite_auto}{{1.3.1}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Estimation Consistency: Stable Gaussian Time Series}{18}}
\newlabel{lemma: hason_bound_time_gauss}{{1.3.2}{18}}
\citation{Basu2015}
\citation{bohm2009shrinkage}
\newlabel{prop:bias_bound}{{1.3.3}{19}}
\citation{bohm2009shrinkage,bohm2008structural}
\citation{bradley2005basic}
\citation{bradley2005basic}
\citation{Basu2015}
\newlabel{eq:ts_rho_mixing}{{1.3.3}{21}}
\newlabel{eqn:var_dto1}{{1.3.4}{21}}
\newlabel{prop:order_bias}{{1.3.4}{21}}
\citation{bickel2008covariance}
\citation{shu2014estimation}
\citation{jung2015graphical}
\newlabel{geo_decay}{{1}{22}}
\newlabel{rho-mixing}{{2}{22}}
\newlabel{var-condition}{{3}{22}}
\citation{bickel2008covariance}
\citation{rothman2009generalized}
\newlabel{prop:variance_bound}{{1.3.5}{23}}
\newlabel{eqn:conc-entrywise}{{1.3.4}{23}}
\newlabel{prop: gauss_prop}{{1.3.6}{23}}
\newlabel{eq:threshold_value}{{1.3.5}{24}}
\newlabel{eq:var_bias_zero}{{1.3.6}{24}}
\citation{rothman2009generalized}
\citation{rothman2009generalized}
\newlabel{prop:consistency}{{1.3.8}{26}}
\citation{rosenblatt1985stationary}
\newlabel{prop:coherance}{{1.3.9}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Spectral Density Estimation of Linear Processes}{27}}
\newlabel{sec:heavy-tail}{{1.4}{27}}
\newlabel{eq:infinite_ma}{{1.4.1}{27}}
\citation{erdHos2012bulk}
\citation{erdHos2012bulk}
\citation{FiniteTimeIdentification2017,wong2017lasso}
\citation{rudelson2013hanson,erdHos2012bulk}
\newlabel{eq:infinity_moving_average}{{1.4.2}{28}}
\newlabel{C1}{{1}{28}}
\newlabel{C2}{{2}{28}}
\newlabel{C3}{{3}{28}}
\newlabel{lemma:heavy_tail_hanson}{{1.4.1}{29}}
\newlabel{lemma:heavy_tail_time_hanson}{{1.4.2}{29}}
\citation{Basu2015}
\newlabel{prop:heavy_tail_bound_variance}{{1.4.3}{30}}
\newlabel{eqn:heavy-tail-conc-entrywise}{{1.4.3}{30}}
\newlabel{prop: linear_prop}{{1.4.4}{31}}
\citation{bohm2009shrinkage}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Simulation Studies}{32}}
\newlabel{sec:simulation}{{1.5}{32}}
\citation{fiecas2014datadriven}
\citation{Kuceyeski2018functional}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Receiver Operating Characeristic (ROC) curves of hard thresholding, lasso and adaptive lasso for recovering coherence network of a $p = 96$ dimensional VAR(1) model using $n = 100$ (top left), $n = 200$ (top right), $n = 400$ (bottom left) and $n = 600$ (bottom right) time series observations.\relax }}{36}}
\newlabel{fig:roc}{{1.1}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Functional Connectivity Analysis with fMRI Data}{36}}
\newlabel{sec:realdata}{{1.6}{36}}
\newlabel{table:rmise-homogeneous-final}{{\caption@xref {table:rmise-homogeneous-final}{ on input line 52}}{37}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Relative Mean Integrated Squared Error (RMISE, in \%) of smoothed periodogram, shrinkage towards a diagonal target and three different thresholding methods - hard thresholding, lasso and adaptive lasso. Results are averaged over $20$ replicates. Standard deviations (also in \%) are reported in parentheses.\relax }}{37}}
\citation{fischl2000measuring}
\citation{bohm2009shrinkage}
\citation{zuo2010growing}
\citation{Utevsky14}
\citation{FRANSSON20081178}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces [top]: Heat maps of absolute coherence matrices (at frequency $0$) obtained from spectral density estimated using [top left] adaptive lasso thresholding and [top right] a shrinkage method. [bottom]: Absolute coherence network among brain regions obtained using adaptive lasso and visualized using BrainNet Viewer. The coherence network estimated by adaptive lasso retains known biological patterns, including presence of bilateral homologues, i.e. strong connectivity between same ROIs in the left and right parts of brain.\relax }}{39}}
\newlabel{fig:realdata}{{1.2}{39}}
\citation{cai2011adaptive}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Discussion}{40}}
\newlabel{sec:discussion}{{1.7}{40}}
\citation{bordier2017graph}
\citation{rudelson2013hanson}
\citation{golub2012matrix}
\@writefile{toc}{\contentsline {section}{\numberline {1.A}Appendix: Proofs for Gaussian Time Series}{42}}
\newlabel{appendix:proof_gaussian}{{1.A}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.A.1}Proof of Lemma 1.3.2\hbox {}}{42}}
\newlabel{eq: hanson1}{{1.A.1}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.A.2}Proof of Proposition 1.3.3\hbox {}}{43}}
\newlabel{eq:mul_dev}{{1.A.2}{43}}
\newlabel{eq:mul_dev1}{{1.A.3}{43}}
\newlabel{eq:mul_dev2}{{1.A.4}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.A.3}Proof for Proposition 1.3.4\hbox {}}{44}}
\newlabel{eq:sum-help}{{1.A.5}{44}}
\@writefile{toc}{\contentsline {paragraph}{Condition 1:}{44}}
\@writefile{toc}{\contentsline {paragraph}{Condition 2:}{44}}
\@writefile{toc}{\contentsline {paragraph}{Condition 3:}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.A.4}Proof of Proposition 1.3.5\hbox {}}{46}}
\newlabel{eq:bound_with_real_img}{{1.A.6}{46}}
\newlabel{eq:realImaginaryParts}{{1.A.6}{46}}
\newlabel{eq: sym_matrix_ine}{{1.A.7}{47}}
\newlabel{eq:mul_three_parts}{{1.A.8}{47}}
\newlabel{eq:qudratic representation}{{1.A.9}{47}}
\newlabel{eq:mul_real_vv}{{1.A.9}{48}}
\newlabel{eq:mul_im_two}{{1.A.10}{49}}
\newlabel{eq:mul_img_final}{{1.A.12}{50}}
\citation{golub2012matrix}
\citation{bickel2008covariance}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.A.5}Proof of Proposition 2.3.7\hbox {}}{51}}
\newlabel{eq:L1_error}{{1.A.15}{51}}
\newlabel{eq:change_bound_to_variance}{{1.A.16}{52}}
\citation{bickel2008covariance}
\citation{bickel2008covariance}
\newlabel{eq:two_parts_L2_norm}{{1.A.17}{53}}
\newlabel{eq:bound_III}{{1.A.18}{54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.A.6}Proof of Proposition 1.3.8\hbox {}}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.A.7}Proof of Proposition 1.3.9\hbox {}}{58}}
\newlabel{eq:single-ratio-bound}{{1.A.19}{59}}
\citation{rudelson2013hanson}
\citation{erdHos2012bulk}
\@writefile{toc}{\contentsline {section}{\numberline {1.B}Appendix: Proofs for Linear Processes}{60}}
\newlabel{Appendix:proof_heavytail}{{1.B}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.B.1}Proof for Lemma 1.4.1\hbox {}}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.B.2}Proof of Proposition 1.4.2\hbox {}}{61}}
\newlabel{eq:sub_gauss_time_hanson_wright_truncated}{{1.B.1}{62}}
\citation{golub2012matrix}
\@writefile{toc}{\contentsline {section}{\numberline {1.C}Appendix: Additional Proofs of Technical Results}{63}}
\newlabel{Appendix:proof_technical_lemmas}{{1.C}{63}}
\newlabel{sec:proof_for_technical_lemmas}{{1.C}{63}}
\newlabel{lemma:q_norm_eq}{{1.C.1}{63}}
\newlabel{lemma:spectral_simple}{{1.C.2}{63}}
\citation{bhatia1990bounds}
\newlabel{eq:eigen_bound}{{1.C.1}{64}}
\newlabel{lemma:orthogonal-cos-sin}{{1.C.3}{64}}
\newlabel{eq:cos_series}{{1.C.2}{65}}
\newlabel{eq:sin_series}{{1.C.3}{65}}
\newlabel{eq:omega_cos}{{1.C.4}{65}}
\newlabel{eq:omega_sin}{{1.C.5}{65}}
\newlabel{eq:case_a_proof}{{1.C.6}{65}}
\newlabel{eq:case_b_proof}{{1.C.7}{65}}
\newlabel{eq:j=k_cos}{{1.C.8}{66}}
\newlabel{eq:case_c_proof}{{1.C.9}{66}}
\newlabel{lemma:maximum_L2_Q}{{1.C.4}{66}}
\citation{Basu2015}
\newlabel{lemma:max-L2-norm}{{1.C.5}{67}}
\newlabel{lemma:max-L2-norm-Y}{{1.C.6}{67}}
\newlabel{lemma:linear_assumption}{{1.C.7}{68}}
\newlabel{eq:finite_sum_auto}{{1.C.14}{68}}
\newlabel{lemma:L2_convergence_truncate}{{1.C.8}{68}}
\newlabel{eq:dct_dominant}{{1.C.15}{69}}
\newlabel{eq:cross_terms_bound}{{1.C.16}{69}}
\newlabel{lemma:spectral_convergence}{{1.C.9}{70}}
\newlabel{eq:autocovariance_dct}{{1.C.18}{71}}
\@writefile{toc}{\contentsline {section}{\numberline {1.D}Appendix: Additional Table and Graphs}{72}}
\newlabel{appendix:more_tables}{{1.D}{72}}
\newlabel{RF1}{73}
\newlabel{table:precision-homogeneous-final}{{\caption@xref {table:precision-homogeneous-final}{ on input line 49}}{73}}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces Precision, Recall, F1 Score ( in $\%$) of three different thresholding methods: hard threshold, lasso and adaptive lasso. \relax }}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Heat map of absolute coherence matrix (at frequency $0$) estimated using adaptive lasso thresholding of averaged periodogram.\relax }}{74}}
\newlabel{fig:realdatafullalasso}{{1.3}{74}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Heat map of absolute coherence matrix (at frequency $0$) estimated using diagonal shrinkage of averaged periodogram.\relax }}{75}}
\newlabel{fig:realdatafullshrinkage}{{1.4}{75}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Large Spectral Density Matrix Estimation for Gaussian Process by Adaptive Thresholding}{76}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{76}}
\newlabel{introduction}{{2.1}{76}}
\newlabel{eq:def_spectral_density}{{2.1.1}{76}}
\citation{fiecas2018spectral,sun2018large}
\citation{fiecas2018spectral}
\citation{sun2018large}
\citation{Basu2015}
\citation{cai2011adaptive}
\citation{donoho1994ideal}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Why Adaptive Thresholding?}{77}}
\citation{bickel2008covariance}
\citation{bickel2008covariance}
\citation{bickel2008covariance}
\citation{bickel2008covariance}
\citation{cai2011adaptive}
\citation{sun2018large}
\citation{sun2018large}
\citation{sun2018large}
\citation{brockwell2013time,rosenblatt1985stationary}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Periodogram Smoothing}{80}}
\newlabel{eq:single_periodogram}{{2.1.3}{80}}
\newlabel{eq:cos_sin_coef}{{2.1.4}{81}}
\newlabel{eq:general_smoothing_estimator}{{2.1.5}{81}}
\newlabel{eq:smoothing estimator}{{2.1.6}{81}}
\citation{brillinger2001time}
\newlabel{eq:def_neb}{{2.1.8}{82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}What Variance should thresholding Value be Adaptive to?}{82}}
\newlabel{assumption:finite_auto}{{2.1.1}{82}}
\citation{cai2011adaptive}
\newlabel{lemma:asy_dis_dft}{{2.1.2}{83}}
\newlabel{eq:limiting_dist}{{2.1.9}{83}}
\citation{cai2011adaptive}
\citation{cai2011adaptive}
\newlabel{eq:variance_of_periodogram}{{2.1.11}{84}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Background and Methods}{85}}
\newlabel{sec:model-methods}{{2.2}{85}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Modified Periodogram and Its Smoothing Estimator}{85}}
\citation{rothman2009generalized}
\citation{rothman2009generalized}
\citation{cai2011adaptive}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Method: Adaptive Thresholding}{87}}
\@writefile{toc}{\contentsline {paragraph}{Estimation of Variance:}{87}}
\newlabel{eq:variance_estimator}{{2.2.9}{88}}
\@writefile{toc}{\contentsline {paragraph}{Adaptive Estimator}{88}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Threshold Selection by Frequency Domain Sample-splitting for Real Part\relax }}{89}}
\newlabel{alg:sample-split-real}{{2}{89}}
\newlabel{al1}{{2}{89}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Theoretical Properties}{89}}
\newlabel{sec:theory}{{2.3}{89}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Bounding the Bias}{89}}
\citation{sun2018large}
\citation{sun2018large}
\@writefile{toc}{\contentsline {paragraph}{Bias for Smoothing Modified Periodogram}{90}}
\newlabel{lemma:bound_deviation}{{2.3.2}{90}}
\@writefile{toc}{\contentsline {paragraph}{Bias for Variance Estimation}{90}}
\newlabel{lemma:theta_bias}{{2.3.3}{91}}
\newlabel{eq:def_delta12}{{2.3.5}{91}}
\newlabel{lemma: variance_ratio_error}{{2.3.4}{91}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Deviation Bound}{91}}
\newlabel{lemma: deviation_variance}{{2.3.5}{92}}
\citation{sun2018large}
\citation{bickel2008covariance}
\citation{sun2018large}
\citation{sun2018large}
\citation{cai2011adaptive}
\citation{cai2011adaptive}
\citation{sun2018large}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Main Results}{95}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Sparse Class}{95}}
\newlabel{eq:sparse_class}{{2.3.19}{95}}
\citation{sun2018large}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Consistency Under Weak Sparsity}{96}}
\newlabel{prop: gauss_prop}{{2.3.7}{96}}
\newlabel{eq:threshold_value}{{2.3.20}{96}}
\citation{sun2018large}
\@writefile{toc}{\contentsline {section}{\numberline {2.A}Proof for Bias Bounding}{97}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.A.1}Proof for Lemma 2.3.2\hbox {}}{97}}
\newlabel{eq:expectation_bias_decomposition}{{2.A.1}{97}}
\newlabel{eq:ada_periodogram_real_part_dif}{{2.A.3}{97}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.A.2}Proof for Lemma 2.3.3\hbox {}}{98}}
\newlabel{eq: dif_liminting_var}{{2.A.7}{98}}
\citation{sun2018large}
\newlabel{eq:help_bound1}{{2.A.8}{99}}
\citation{rudelson2013hanson}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.A.3}Proof for Lemma 2.3.4\hbox {}}{101}}
\@writefile{toc}{\contentsline {section}{\numberline {2.B}Proof for Deviation Bound}{101}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.B.1}Proof for Lemma 2.3.5\hbox {}}{101}}
\newlabel{eq:theta_concentration1}{{2.B.1}{101}}
\@writefile{toc}{\contentsline {section}{\numberline {2.C}Technical Lemmas}{102}}
\newlabel{sec:technical_lemmas}{{2.C}{102}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.C.1}\bf  Fourth Moments of Multivariate Normal Distribution}{102}}
\newlabel{subsec: gaussian_fourth_moments}{{2.C.1}{102}}
\newlabel{eq:gauss_fourth_moment}{{2.C.1}{102}}
\citation{rencher2008linear}
\citation{rudelson2013hanson}
\citation{zygmund2002trigonometric}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.C.2}Variance of }{103}}
\newlabel{lemma: variant_hanson_wright}{{2.C.1}{103}}
\@writefile{toc}{\contentsline {section}{\numberline {2.D}Technical Results for Toeplitz Matrixz}{103}}
\newlabel{lemma:sin_cos_seq_sum}{{2.D.1}{103}}
\newlabel{lemma: bound_toeplitz}{{2.D.2}{104}}
\citation{cai2011adaptive}
\newlabel{eq:similarity_real_im}{{2.D.3}{105}}
\@writefile{toc}{\contentsline {section}{\numberline {2.E}An Example Explaining Why We Modify the Periodogram}{105}}
\newlabel{sec:counter_example}{{2.E}{105}}
\citation{vasilescu2002multilinear}
\citation{cichocki2013tensor}
\citation{austin2016parallel}
\citation{sun2008incremental}
\citation{kolda2008scalable}
\citation{kolda2009tensor}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Low-Rank Tucker Approximation of a Tensor From Streaming Data}{107}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{107}}
\citation{muthukrishnan2005data}
\citation{malik2018low}
\citation{kolda2009tensor}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Background and Related Work}{110}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Notation}{110}}
\@writefile{toc}{\contentsline {subsubsection}{Tail energy}{111}}
\@writefile{toc}{\contentsline {subsubsection}{Kronecker and Khatri-Rao product}{111}}
\newlabel{kronecker}{{3.2.1}{111}}
\@writefile{toc}{\contentsline {subsubsection}{Tensor basics}{111}}
\@writefile{toc}{\contentsline {subsubsection}{Tensor unfoldings}{112}}
\newlabel{eq:F_norm_equivalent}{{3.2.2}{112}}
\@writefile{toc}{\contentsline {subsubsection}{Tensor rank}{112}}
\@writefile{toc}{\contentsline {subsubsection}{Tensor contractions}{112}}
\citation{kolda2009tensor}
\citation{de2000multilinear,tucker1966some}
\citation{de2000multilinear,tucker1966some}
\citation{de2000multilinear,tucker1966some}
\citation{zhou2014decomposition,battaglino2019faster}
\citation{halko2011finding}
\newlabel{eq: tensor_product_association}{{3.2.1}{113}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Tucker Approximation}{113}}
\newlabel{eq:tucker_optimization}{{3.2.3}{113}}
\@writefile{toc}{\contentsline {subsubsection}{HOSVD}{113}}
\citation{vannieuwenhoven2012new}
\citation{vannieuwenhoven2012new}
\citation{de2000multilinear}
\citation{de2000multilinear}
\citation{de2000multilinear}
\newlabel{alg:hosvd}{{1}{114}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Higher order singular value decomposition (HOSVD) \cite  {de2000multilinear,tucker1966some} \relax }}{114}}
\@writefile{toc}{\contentsline {subsubsection}{ST-HOSVD}{114}}
\citation{malik2018low}
\citation{malik2018low}
\citation{malik2018low}
\citation{malik2018low}
\@writefile{toc}{\contentsline {subsubsection}{HOOI}{115}}
\newlabel{alg:hooi}{{2}{115}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Higher order orthogonal iteration (HOOI) \cite  {de2000multilinear} \relax }}{115}}
\newlabel{eq:factor-update}{{3.2.4}{115}}
\newlabel{eq:core_update}{{3.2.5}{115}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Previous Work}{115}}
\newlabel{sec: previous_work}{{3.2.3}{115}}
\citation{halko2011finding,woodruff2014sketching}
\citation{tropp2018more,tropp2019streaming}
\citation{wang2015fast,battaglino2018practical}
\citation{tsourakakis2010mach}
\citation{baskaran2012efficient,zhou2014decomposition,austin2016parallel,kaya2016high,li2015input,battaglino2019faster}
\citation{oymak2015universality}
\citation{woolfe2008fast}
\citation{achlioptas2003database,li2006very}
\citation{sun2018tensor}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Dimension Reduction Maps}{117}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Dimension Reduction Map}{117}}
\citation{sun2018tensor}
\citation{rudelson2012row}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Tensor Random Projection}{118}}
\newlabel{s-trp}{{3.3.2}{118}}
\newlabel{eq:TRP}{{3.3.1}{118}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Performance of Different Dimension Reduction Maps: We compare the storage cost and the computational cost of applying a DRM mapping $\mathbb  {R}^{I^N}$ to $\mathbb  {R}^k$ to a dense tensor in $\mathbb  {R}^{I^N}$. Here $\mu $ is the sparse factor for sparse random projection. The TRP considered here is composed of Gaussian DRMs. \relax }}{118}}
\newlabel{tbl: random_map}{{3.1}{118}}
\citation{tropp2018more}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Algorithms for Tucker approximation}{119}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Tensor compression via sketching}{119}}
\newlabel{sec:sketch}{{3.4.1}{119}}
\@writefile{toc}{\contentsline {paragraph}{The Tucker sketch}{119}}
\newlabel{sketches}{{3.4.1}{119}}
\citation{arora2009computational}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Tucker Sketch\relax }}{120}}
\newlabel{alg:tensor_sketch}{{3}{120}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Low-Rank Approximation}{121}}
\newlabel{eqn:qr}{{3.4.2}{121}}
\newlabel{eq:x_tilde}{{3.4.3}{121}}
\newlabel{eq: two-pass}{{3.4.4}{121}}
\newlabel{alg:two_pass_low_rank_appro}{{4}{122}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Two Pass Sketch and Low Rank Recovery \relax }}{122}}
\@writefile{toc}{\contentsline {paragraph}{One-Pass Approximation}{122}}
\citation{malik2018low}
\citation{tropp2019streaming}
\newlabel{alg:one_pass_low_rank_appro}{{5}{123}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces One Pass Sketch and Low Rank Recovery \relax }}{123}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces  Computational Complexity of 5\hbox {} on tensor $\T {X} \in \mathbb  {R}^{I \times \dots  \times I}$ with parameters $(k,s)$, using a TRP composed of Gaussian DRMs inside the Tucker sketch. By far the majority of the time is spent sketching the tensor $\T {X}$. \relax }}{123}}
\newlabel{tbl: time-complexity}{{3.2}{123}}
\citation{vannieuwenhoven2012new}
\citation{ballester2019tthresh}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Fixed-Rank Approximation}{124}}
\newlabel{sec:fixed_rank}{{3.4.3}{124}}
\newlabel{lemma: equivalance_one_pass}{{3.4.1}{124}}
\newlabel{alg:one_pass_fix_rank_appro}{{6}{125}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Fixed rank approximation \relax }}{125}}
\newlabel{line:core-decom}{{1}{125}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Guarantees}{125}}
\newlabel{sec:theory}{{3.5}{125}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Low rank approximation}{125}}
\newlabel{thm:low_rank_err_two_pass}{{3.5.1}{125}}
\newlabel{thm:low_rank_err}{{3.5.2}{126}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Fixed rank approximation}{127}}
\newlabel{thm:fix_rank_err}{{3.5.3}{127}}
\newlabel{eq-fixed-rank-bound}{{3.5.3}{127}}
\citation{malik2018low}
\citation{malik2018low}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Proof sketch}{128}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \textit  {Different DRMs perform similarly.} We approximate 3D synthetic tensors (see 3.6.1\hbox {}) with $I = 600$, using our one-pass algorithm with $r = 5$ and varying $k$ ($s = 2k+1$), using a variety of DRMs in the Tucker sketch: Gaussian, SSRFT, Gaussian TRP, or Sparse TRP.  \relax }}{129}}
\newlabel{fig:vary-k-600}{{3.1}{129}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \textit  {Two-pass improves on one-pass.} We approximate 3D synthetic tensors (see 3.6.1\hbox {}) with $I = 600$, using our one-pass and two-pass algorithms with $r = 5$ and varying $k$ ($s = 2k+1$), using the Gaussian TRP in the Tucker sketch.  \relax }}{129}}
\newlabel{fig:vary-k-600-compare}{{3.2}{129}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  \textit  {Faster approximations.} We approximate 3D synthetic tensors with $I = 600$ generated as described in 3.6.1\hbox {}, using HOOI and our one-pass and two-pass algorithms with $r = 5$ for a few different $k$ ($s = 2k+1$). \relax }}{130}}
\newlabel{fig:run_time}{{3.3}{130}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Numerical Experiments}{130}}
\newlabel{s-experiments}{{3.6}{130}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Synthetic experiments}{131}}
\newlabel{s-synthetic-data}{{3.6.1}{131}}
\citation{malik2018low}
\citation{malik2018low}
\citation{malik2018low}
\@writefile{toc}{\contentsline {subsubsection}{Different dimension reduction maps perform similarly}{132}}
\@writefile{toc}{\contentsline {subsubsection}{A second pass reduces error}{132}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \textit  {Approximations improve with more memory: synthetic data.} We approximate 3D synthetic tensors (see 3.6.1\hbox {}) with $I = 300$, using T.-TS and our one-pass and two-pass algorithms with the Gaussian TRP to produce approximations with equal ranks $r=10$. Notice every marker on the plot corresponds to a 2700$\times $ compression!\relax }}{133}}
\newlabel{fig:vary-memory}{{3.4}{133}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces \textit  {Approximations improves with more memory: real data.} We approximate aerosol absorption and combustion data using our one-pass and two-pass algorithms with the Gaussian TRP. We compare three target ranks ($r/I = 0.125,0.1,0.067$) for the former, and use the same target rank ($r/I = 0.1$) for each measured quantity in the combustion dataset. Notice $r/I = 0.1$ gives a hundred-fold compression! \relax }}{133}}
\newlabel{fig:climate}{{3.5}{133}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \textit  {Video Scene Classification} ($2200 \times 1080 \times 1980$): We classify frames from the video data from \cite  {malik2018low} (collected as a third order tensor with size $2200 \times 1080 \times 1980$) using $K$-means with $K$=3 on vectors computed using four different methods. $s = 2k+1$ throughout. 1) The linear sketch along the time dimension (Row 1). 2-3) the Tucker factor along the time dimension, computed via our two-pass (Row 2) and one-pass (Row 3) algorithms. 4) The Tucker factor along the time dimension, computed via our one-pass (Row 4) algorithm \relax }}{134}}
\newlabel{fig:video}{{3.6}{134}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces \textit  {Visualizing Video Recovery:} Original frame (left); approximation by two-pass sketch (middle); approximation by one-pass sketch (right). \relax }}{134}}
\newlabel{fig:Frame500}{{3.7}{134}}
\@writefile{toc}{\contentsline {subsubsection}{Improvement on state-of-the-art}{134}}
\citation{malik2018low}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces \textit  {Visualizing Combustion Simulation:} All four figures show a slice of the temperature data along the first dimension. The approximation uses $\mathbf  {r} = (281,25,25)$, $\mathbf  {k} = (562,50,50)$, $\mathbf  {s} = (1125, 101, 101)$, with the Gaussian TRP in the Tucker sketch.\relax }}{135}}
\newlabel{fig:T100}{{3.8}{135}}
\citation{hurrell2013community,kay2015community}
\citation{lapointe2015differential}
\citation{malik2018low}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Applications}{136}}
\newlabel{s-real-data}{{3.6.2}{136}}
\@writefile{toc}{\contentsline {subsubsection}{Data compression}{136}}
\citation{malik2018low}
\@writefile{toc}{\contentsline {subsubsection}{Video scene classification}{137}}
\citation{malik2018low}
\citation{malik2018low}
\citation{malik2018low}
\citation{malik2018low}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Conclusion}{138}}
\citation{de2008tensor}
\citation{vannieuwenhoven2012new}
\@writefile{toc}{\contentsline {section}{\numberline {3.A}Proof of Main Results}{140}}
\newlabel{appendix:proof-main-result}{{3.A}{140}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.A.1}Error bound for the two pass approximation Algorithm 4\hbox {}}{140}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.A.2}Error bound for the one pass approximation Algorithm 5\hbox {}}{140}}
\citation{tropp2017practical}
\newlabel{eq:inner_zero}{{3.A.3}{141}}
\newlabel{eq:error_decom}{{3.A.4}{141}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.A.3}Error bound for the fixed rank approximation Algorithm 6\hbox {}}{142}}
\@writefile{toc}{\contentsline {section}{\numberline {3.B}Probabilistic Analysis of Core Sketch Error}{142}}
\newlabel{eq: def-proj-Q}{{3.B.1}{142}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.B.1}Decomposition of Core Approximation Error}{143}}
\newlabel{lemma:core_error_decomposition}{{3.B.1}{143}}
\newlabel{eq:def_each_part}{{3.B.2}{143}}
\newlabel{eq:core_err_decom}{{3.B.3}{144}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.B.2}Probabilistic Core Error Bound}{144}}
\newlabel{lemma:err_core_sketch}{{3.B.2}{145}}
\newlabel{eq:factor-matrix-error-bounds-core-error}{{3.B.4}{145}}
\newlabel{eq:inner_prod2}{{3.B.5}{145}}
\@writefile{toc}{\contentsline {section}{\numberline {3.C}Proof of fixed rank approximation lemma}{146}}
\newlabel{appendix: proof-fix-rank-lemma}{{3.C}{146}}
\citation{halko2011finding}
\@writefile{toc}{\contentsline {section}{\numberline {3.D}Technical Lemmas}{147}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.D.1}Random projections of matrices}{147}}
\newlabel{s-matrix-projections}{{3.D.1}{147}}
\newlabel{lemma:expectation_inverse_gaussian}{{3.D.1}{147}}
\newlabel{lemma:sketchy_column_space_err}{{3.D.2}{147}}
\@writefile{toc}{\contentsline {section}{\numberline {3.E}More Algorithms}{148}}
\newlabel{appendix:more_algorithms}{{3.E}{148}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Linear Update to Sketches\relax }}{148}}
\newlabel{alg:linear_update}{{7}{148}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces Sketching in Distributed Setting\relax }}{148}}
\newlabel{alg:sketch_distributed}{{8}{148}}
\@writefile{toc}{\contentsline {section}{\numberline {3.F}Scrambled Subsampled Randomized Fourier Transform}{148}}
\newlabel{appendix: ssrft}{{3.F}{148}}
\citation{boutsidis2013improved,tropp2011improved,ailon2009fast}
\citation{2017arXiv171209473D}
\citation{malik2018low}
\citation{malik2018low}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces Scrambled Subsampled Randomized Fourier Transform (Row Linear Transform)\relax }}{149}}
\newlabel{alg:ssrft}{{9}{149}}
\citation{cormode2008finding}
\citation{clarkson2017low}
\citation{malik2018low}
\citation{malik2018low}
\@writefile{toc}{\contentsline {section}{\numberline {3.G}TensorSketch}{150}}
\newlabel{appendix: TensorSketch}{{3.G}{150}}
\@writefile{toc}{\contentsline {paragraph}{CountSketch}{150}}
\@writefile{toc}{\contentsline {paragraph}{TensorSketch}{150}}
\citation{cormode2008finding}
\citation{malik2018low}
\newlabel{eq: tucker-stage-1}{{3.G.1}{151}}
\newlabel{eq: tucker-stage-2}{{3.G.2}{151}}
\newlabel{eq:tensorsketch}{{3.G.3}{151}}
\@writefile{toc}{\contentsline {section}{\numberline {3.H}More Numerics}{151}}
\newlabel{appendix:more_result}{{3.H}{151}}
\newlabel{appendix:more_real_data_result}{{3.H}{151}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces We approximate 3D synthetic tensors (see 3.6.1\hbox {}) with $I = 400$, using our one-pass algorithm with $r = 5$ and varying $k$ ($s = 2k+1$), using a variety of DRMs in the Tucker sketch: Gaussian, SSRFT, Gaussian TRP, or Sparse TRP.\relax }}{152}}
\newlabel{fig:vary-k-400-app}{{3.9}{152}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces We approximate 3D synthetic tensors (see 3.6.1\hbox {}) with $I = 200$, using our one-pass algorithm with $r = 5$ and varying $k$ ($s = 2k+1$), using a variety of DRMs in the Tucker sketch: Gaussian, SSRFT, Gaussian TRP, or Sparse TRP.\relax }}{152}}
\newlabel{fig:vary-k-200-app}{{3.10}{152}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces We approximate 3D synthetic tensors (see 3.6.1\hbox {}) with $I = 400$, using our one-pass and two-pass algorithms with $r = 5$ and varying $k$ ($s = 2k+1$), using the Gaussian TRP in the Tucker sketch.\relax }}{153}}
\newlabel{fig:vary-k-400-compare-app}{{3.11}{153}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces We approximate 3D synthetic tensors (see 3.6.1\hbox {}) with $I = 200$, using our one-pass and two-pass algorithms with $r = 5$ and varying $k$ ($s = 2k+1$), using the Gaussian TRP in the Tucker sketch.\relax }}{153}}
\newlabel{fig:vary-k-200-compare-app}{{3.12}{153}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces We approximate the net radiative flux and dust aerosol burden data using our one-pass and two-pass algorithms using Gaussian TRP. We compare the performance under different ranks ($r/I = 0.125,0.2,0.067$). The dataset comes from the CESM CAM. The dust aerosol burden measures the amount of aerosol contributed by the dust. The net radiative flux determines the energy received by the earth surface through radiation. \relax }}{154}}
\newlabel{fig:srfrad_burden_dust}{{3.13}{154}}
\citation{wright2009robust,buhler2002finding,allen2014sparse,bingham2001random,fradkin2003experiments,halko2011finding,wang2012semi,jegou2008hamming}
\citation{papadimitriou2000latent}
\citation{sahin2005prism,kaski1998dimensionality}
\citation{liu2006random}
\citation{woolfe2008fast,tropp2017practical}
\citation{yurtsever2017sketchy}
\citation{Tropp2019-SketchingScientificSimulation,sun2019low}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Tensor Random Projection for Low Memory Dimension Reduction}{155}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{155}}
\newlabel{eq:low_dim_mapping}{{4.1.1}{155}}
\citation{arriaga2006algorithmic}
\citation{halko2011finding}
\citation{achlioptas2003database}
\citation{li2006very,ailon2006approximate,bourgain2015toward}
\newlabel{lem:gauss-rp-vector}{{4.1.1}{156}}
\newlabel{eq:gauss_random_projection}{{4.1.4}{156}}
\newlabel{lemma:gauss-rp-matrix}{{4.1.2}{156}}
\newlabel{eq:gauss_col_preservation}{{4.1.5}{156}}
\@writefile{toc}{\contentsline {paragraph}{Memory Efficient Random Projection}{156}}
\citation{rudelson2012row}
\citation{battaglino2018practical}
\citation{wang2015fast}
\citation{woodruff2014sketching}
\citation{matsumoto1998mersenne}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Notation}{158}}
\newlabel{khatri-rao}{{4.1.6}{158}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Tensor Random Projection}{158}}
\citation{achlioptas2003database}
\citation{li2006very}
\newlabel{eq:TRP}{{4.2.1}{159}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Main Theory}{160}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Bias and Variance}{160}}
\newlabel{thm: norm-preserve}{{4.3.1}{160}}
\newlabel{eq:lemma-invariant-length-statement}{{4.3.1}{160}}
\newlabel{thm:variance}{{4.3.2}{160}}
\newlabel{cor:pairwise-distance-unbias-variance}{{4.3.3}{161}}
\newlabel{lem:inner_product_TRP}{{4.3.4}{161}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Asymptotic Behavior}{162}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Finite Sample Bound?}{162}}
\newlabel{prop: N-2-bound}{{4.3.5}{162}}
\citation{schacke2013kronecker}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Column Space Preservation}{163}}
\citation{schacke2013kronecker}
\citation{achlioptas2003database}
\citation{li2006very}
\citation{achlioptas2003database}
\citation{li2006very}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Experiment}{164}}
\newlabel{sec:simulation}{{4.4}{164}}
\citation{halko2011finding,woodruff2014sketching}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Isometry quality for simulated and MNIST data. The left two plots show results for Gaussian and Very Sparse RP, TRP, TRP(5) respectively applied to $n = 20$ standard normal data vectors in $\mathbb  {R}^{2500}$. The right two plots show the same for 50 MNIST image vectors in $\mathbb  {R}^{784}$. The dashed line shows the error two standard deviations from the average ratio.\relax }}{165}}
\newlabel{fig:main}{{4.1}{165}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces RMSE for the estimate of the pairwise inner product of the MNIST data, where standard error is in the parentheses.\relax }}{165}}
\newlabel{tbl:mnist_inner_prod}{{4.1}{165}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Application: Sketching}{166}}
\newlabel{appendix:sketching}{{4.5}{166}}
\citation{de2000multilinear,wang2015fast}
\citation{kolda2009tensor}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10}{\ignorespaces Tensor Sketching with Variance Reduction\relax }}{167}}
\newlabel{alg:var-red-structure-sketching}{{10}{167}}
\@writefile{toc}{\contentsline {paragraph}{Experimental Setup}{167}}
\@writefile{toc}{\contentsline {paragraph}{Result}{168}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Relative Error for the low-rank tensor unfolding approximation: \textit  {we compare the relative errors for low-rank tensor approximation with different input size: 2-D ($900 \times 900$), 3-D ($400 \times 400 \times 400$), 4-D ($100 \times 100 \times 100 \times 100$). In each setting, we compare the performance of Gaussian RP, TRP, and $\textup  {TRP}_5$. The dashed line stands for the 95\% confidence interval.}\relax }}{169}}
\newlabel{fig:col_matrix}{{4.2}{169}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Conclusion}{169}}
\@writefile{toc}{\contentsline {section}{\numberline {4.A}Proof for Bias and Variance Analysis}{169}}
\newlabel{sec:appendix_proof}{{4.A}{169}}
\newlabel{eq:row-length}{{4.A.1}{171}}
\newlabel{eq:TRP_fourth_moment}{{4.A.1}{173}}
\newlabel{eq: inner_prod_unbias}{{4.A.2}{174}}
\newlabel{eq:inner_prod_second_moment}{{4.A.2}{175}}
\@writefile{toc}{\contentsline {section}{\numberline {4.B}More Simulation Results}{178}}
\newlabel{appendix:more_result}{{4.B}{178}}
\@writefile{toc}{\contentsline {paragraph}{Pairwise Distance Estimation}{178}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Average ratio of the pairwise distance for simulation data using Gaussian RP: \textit  {The plots correspond to the simulation for Gaussian RP, TRP, $\textup  {TRP}_5$ respectively with $n = 20, d = 2500, 10000, 40000$ and each data vector comes from $N(\mathbf  {0}, \mathbf  {I})$. The dashed line represents the error bar 2 standard deviation away from the average ratio.}\relax }}{178}}
\newlabel{fig:gaussian}{{4.3}{178}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Average ratio of the pairwise distance for simulation data using Sparse RP: \textit  {The plots correspond to the simulation for Sparse RP, TRP, $\textup  {TRP}_5$ respectively with $n = 20, d = 2500, 10000, 40000$ and each data vector comes from $N(\mathbf  {0}, \mathbf  {I})$. The dashed line represents the error bar 2 standard deviation away from the average ratio.}\relax }}{178}}
\newlabel{fig:sparse}{{4.4}{178}}
\@writefile{toc}{\contentsline {paragraph}{Pairwise Cosine Similarity Estimation}{178}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Average ratio of the pairwise distance for simulation data using Very Sparse RP: \textit  {The plots correspond to the simulation for Very Sparse RP, TRP, $\textup  {TRP}_5$ respectively with $n = 20, d = 2500, 10000, 40000$ and each data vector comes from $N(\mathbf  {0}, \mathbf  {I})$. The dashed line represents the error bar 2 standard deviation away from the average ratio.}\relax }}{179}}
\newlabel{fig:very_sparse}{{4.5}{179}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Average ratio of the pairwise distance for simulation data using: \textit  {The plots correspond to the simulation for Gaussian, Sparase, Very Sparse RP, TRP, $\textup  {TRP}_5$ respectively with $n = 20, d = d_1d_2d_3 = 50 \times 50 \times 50 = 125000$ and each data vector comes from $N(\mathbf  {0}, \mathbf  {I})$. The dashed line represents the error bar 2 standard deviation away from the average ratio.}\relax }}{179}}
\newlabel{fig:triple_krao}{{4.6}{179}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces RMSE for the estimate of the pairwise inner product of the simulation data ($d = 10000, k = 50, n = 100 $), where standard error is in the parentheses. \relax }}{179}}
\newlabel{tbl:sim_inner_prod}{{4.2}{179}}
\@writefile{toc}{\contentsline {section}{\numberline {4.C}Appendix: Finite Sample Bound}{180}}
\newlabel{def:generalized-sub-exponential-mc}{{4.C.1}{180}}
\newlabel{eq:constant_in-sub-exponential}{{4.C.2}{181}}
\newlabel{lemma:inner-product}{{4.C.1}{182}}
\newlabel{eq:inner-bound}{{4.C.3}{182}}
\citation{rudelson2013hanson}
\@writefile{toc}{\contentsline {section}{\numberline {4.D}Technical Lemmas}{183}}
\newlabel{def:sub-gaussian}{{4.D.1}{183}}
\newlabel{lemma:hanson_wright}{{4.D.1}{183}}
\newlabel{lemma:hanson-wright-sub-exponential}{{4.D.2}{183}}
\newlabel{eq:sub-exponential-moment-condition}{{4.D.2}{183}}
\citation{buhler2002finding}
\newlabel{lemma:hanson-wright-sub-exponential-moment}{{4.D.3}{184}}
\bibstyle{plainnat}
\bibdata{biblio1,biblio2,biblio3,biblio4}
\bibcite{achlioptas2003database}{{1}{2003}{{Achlioptas}}{{}}}
\bibcite{ailon2006approximate}{{2}{2006}{{Ailon and Chazelle}}{{}}}
\bibcite{ailon2009fast}{{3}{2009}{{Ailon and Chazelle}}{{}}}
\bibcite{allen2014sparse}{{4}{2014}{{Allen-Zhu et~al.}}{{Allen-Zhu, Gelashvili, Micali, and Shavit}}}
\bibcite{arora2009computational}{{5}{2009}{{Arora and Barak}}{{}}}
\bibcite{arriaga2006algorithmic}{{6}{2006}{{Arriaga and Vempala}}{{}}}
\bibcite{austin2016parallel}{{7}{2016}{{Austin et~al.}}{{Austin, Ballard, and Kolda}}}
\bibcite{ballester2019tthresh}{{8}{2019}{{Ballester-Ripoll et~al.}}{{Ballester-Ripoll, Lindstrom, and Pajarola}}}
\bibcite{baskaran2012efficient}{{9}{2012}{{Baskaran et~al.}}{{Baskaran, Meister, Vasilache, and Lethin}}}
\bibcite{Basu2015}{{10}{2015}{{Basu and Michailidis}}{{}}}
\bibcite{battaglino2018practical}{{11}{2018}{{Battaglino et~al.}}{{Battaglino, Ballard, and Kolda}}}
\bibcite{battaglino2019faster}{{12}{2019}{{Battaglino et~al.}}{{Battaglino, Ballard, and Kolda}}}
\bibcite{bhatia1990bounds}{{13}{1990}{{Bhatia et~al.}}{{Bhatia, Elsner, and Krause}}}
\bibcite{bickel2008covariance}{{14}{2008}{{Bickel and Levina}}{{}}}
\bibcite{bingham2001random}{{15}{2001}{{Bingham and Mannila}}{{}}}
\bibcite{bohm2008structural}{{16}{2008}{{B{\"o}hm and Von~Sachs}}{{}}}
\bibcite{bohm2009shrinkage}{{17}{2009}{{B{\"o}hm and von Sachs}}{{}}}
\bibcite{bordier2017graph}{{18}{2017}{{Bordier et~al.}}{{Bordier, Nicolini, and Bifone}}}
\bibcite{bourgain2015toward}{{19}{2015}{{Bourgain et~al.}}{{Bourgain, Dirksen, and Nelson}}}
\bibcite{boutsidis2013improved}{{20}{2013}{{Boutsidis and Gittens}}{{}}}
\bibcite{bowyer2016coherence}{{21}{2016}{{Bowyer}}{{}}}
\bibcite{bradley2005basic}{{22}{2005}{{Bradley}}{{}}}
\bibcite{brillinger1981time}{{23}{1981}{{Brillinger}}{{}}}
\bibcite{brillinger2001time}{{24}{2001}{{Brillinger}}{{}}}
\bibcite{brockwell2013time}{{25}{2013}{{Brockwell and Davis}}{{}}}
\bibcite{buhler2002finding}{{26}{2002}{{Buhler and Tompa}}{{}}}
\bibcite{cai2012minimax}{{27}{2012}{{Cai and Zhou}}{{}}}
\bibcite{cai2016rates}{{28}{2016}{{Cai et~al.}}{{Cai, Ren, and Zhou}}}
\bibcite{cai2011adaptive}{{29}{2011}{{Cai and Liu}}{{}}}
\bibcite{cichocki2013tensor}{{30}{2013}{{Cichocki}}{{}}}
\bibcite{clarkson2017low}{{31}{2017}{{Clarkson and Woodruff}}{{}}}
\bibcite{cormode2008finding}{{32}{2008}{{Cormode and Hadjieleftheriou}}{{}}}
\bibcite{dahlhaus2003causality}{{33}{2003}{{Dahlhaus and Eichler}}{{}}}
\bibcite{dahlhaus1997identification}{{34}{1997}{{Dahlhaus et~al.}}{{Dahlhaus, Eichler, and Sandk{\"u}hler}}}
\bibcite{de2000multilinear}{{35}{2000}{{De~Lathauwer et~al.}}{{De~Lathauwer, De~Moor, and Vandewalle}}}
\bibcite{2017arXiv171209473D}{{36}{2017}{{{Diao} et~al.}}{{{Diao}, {Song}, {Sun}, and {Woodruff}}}}
\bibcite{donoho1994ideal}{{37}{1994}{{Donoho and Johnstone}}{{}}}
\bibcite{eichler2007frequency}{{38}{2007}{{Eichler}}{{}}}
\bibcite{erdHos2012bulk}{{39}{2012}{{Erd{\H {o}}s et~al.}}{{Erd{\H {o}}s, Yau, and Yin}}}
\bibcite{euan2016hierarchical}{{40}{2016}{{Euan et~al.}}{{Euan, Ombao, and Ortega}}}
\bibcite{FiniteTimeIdentification2017}{{41}{2018}{{Faradonbeh et~al.}}{{Faradonbeh, Tewari, and Michailidis}}}
\bibcite{fiecas2016dynamic}{{42}{2016}{{Fiecas and Ombao}}{{}}}
\bibcite{fiecas2014datadriven}{{43}{2014}{{Fiecas and von Sachs}}{{}}}
\bibcite{fiecas2018spectral}{{44}{2018}{{Fiecas et~al.}}{{Fiecas, Leng, Liu, and Yu}}}
\bibcite{fradkin2003experiments}{{45}{2003}{{Fradkin and Madigan}}{{}}}
\bibcite{golub2012matrix}{{46}{2012}{{Golub and Van~Loan}}{{}}}
\bibcite{granger1969investigating}{{47}{1969}{{Granger}}{{}}}
\bibcite{halko2011finding}{{48}{2011}{{Halko et~al.}}{{Halko, Martinsson, and Tropp}}}
\bibcite{hurrell2013community}{{49}{2013}{{Hurrell et~al.}}{{Hurrell, Holland, Gent, Ghan, Kay, Kushner, Lamarque, Large, Lawrence, Lindsay, et~al.}}}
\bibcite{jegou2008hamming}{{50}{2008}{{Jegou et~al.}}{{Jegou, Douze, and Schmid}}}
\bibcite{jung2015learning}{{51}{2015}{{Jung}}{{}}}
\bibcite{jung2015graphical}{{52}{2015}{{Jung et~al.}}{{Jung, Hannak, and Goertz}}}
\bibcite{kaski1998dimensionality}{{53}{1998}{{Kaski}}{{}}}
\bibcite{kay2015community}{{54}{2015}{{Kay et~al.}}{{Kay, Deser, Phillips, Mai, Hannay, Strand, Arblaster, Bates, Danabasoglu, Edwards, et~al.}}}
\bibcite{kaya2016high}{{55}{2016}{{Kaya and U{\c {c}}ar}}{{}}}
\bibcite{kolda2009tensor}{{56}{2009}{{Kolda and Bader}}{{}}}
\bibcite{kolda2008scalable}{{57}{2008}{{Kolda and Sun}}{{}}}
\bibcite{Kuceyeski2018functional}{{58}{2018}{{Kuceyeski et~al.}}{{Kuceyeski, Jamison, Owen, Raj, and Mukherjee}}}
\bibcite{lapointe2015differential}{{59}{2015}{{Lapointe et~al.}}{{Lapointe, Savard, and Blanquart}}}
\bibcite{ledoit2004well}{{60}{2004}{{Ledoit and Wolf}}{{}}}
\bibcite{li2015input}{{61}{2015}{{Li et~al.}}{{Li, Battaglino, Perros, Sun, and Vuduc}}}
\bibcite{li2006very}{{62}{2006}{{Li et~al.}}{{Li, Hastie, and Church}}}
\bibcite{liu2006random}{{63}{2006}{{Liu et~al.}}{{Liu, Kargupta, and Ryan}}}
\bibcite{malik2018low}{{64}{2018}{{Malik and Becker}}{{}}}
\bibcite{matsumoto1998mersenne}{{65}{1998}{{Matsumoto and Nishimura}}{{}}}
\bibcite{muthukrishnan2005data}{{66}{2005}{{Muthukrishnan et~al.}}{{}}}
\bibcite{ombao2001smoothing}{{67}{2001}{{Ombao et~al.}}{{Ombao, Raz, Strawderman, and von Sachs}}}
\bibcite{oymak2015universality}{{68}{2015}{{Oymak and Tropp}}{{}}}
\bibcite{papadimitriou2000latent}{{69}{2000}{{Papadimitriou et~al.}}{{Papadimitriou, Raghavan, Tamaki, and Vempala}}}
\bibcite{rencher2008linear}{{70}{2008}{{Rencher and Schaalje}}{{}}}
\bibcite{rosenblatt1985stationary}{{71}{1985}{{Rosenblatt}}{{}}}
\bibcite{rothman2009generalized}{{72}{2009}{{Rothman et~al.}}{{Rothman, Levina, and Zhu}}}
\bibcite{rudelson2012row}{{73}{2012}{{Rudelson}}{{}}}
\bibcite{rudelson2013hanson}{{74}{2013}{{Rudelson and Vershynin}}{{}}}
\bibcite{sahin2005prism}{{75}{2005}{{Sahin et~al.}}{{Sahin, Gulbeden, Emek{\c {c}}i, Agrawal, and El~Abbadi}}}
\bibcite{schacke2013kronecker}{{76}{2013}{{Sch{\"a}cke}}{{}}}
\bibcite{shu2014estimation}{{77}{2014}{{Shu and Nan}}{{}}}
\bibcite{sun2008incremental}{{78}{2008}{{Sun et~al.}}{{Sun, Tao, Papadimitriou, Yu, and Faloutsos}}}
\bibcite{sun2018tensor}{{79}{2018{a}}{{Sun et~al.}}{{Sun, Guo, Tropp, and Udell}}}
\bibcite{sun2018large}{{80}{2018{b}}{{Sun et~al.}}{{Sun, Li, Kuceyeski, and Basu}}}
\bibcite{sun2019low}{{81}{2019}{{Sun et~al.}}{{Sun, Guo, Luo, Tropp, and Udell}}}
\bibcite{Tropp2019-SketchingScientificSimulation}{{82}{2019{a}}{{Tropp et~al.}}{{Tropp, Yurtsever, Udell, and Cevher}}}
\bibcite{tropp2011improved}{{83}{2011}{{Tropp}}{{}}}
\bibcite{tropp2017practical}{{84}{2017}{{Tropp et~al.}}{{Tropp, Yurtsever, Udell, and Cevher}}}
\bibcite{tropp2018more}{{85}{2018}{{Tropp et~al.}}{{Tropp, Yurtsever, Udell, and Cevher}}}
\bibcite{tropp2019streaming}{{86}{2019{b}}{{Tropp et~al.}}{{Tropp, Yurtsever, Udell, and Cevher}}}
\bibcite{tsourakakis2010mach}{{87}{2010}{{Tsourakakis}}{{}}}
\bibcite{tucker1966some}{{88}{1966}{{Tucker}}{{}}}
\bibcite{van2016lecture}{{89}{2016}{{van~de Geer}}{{}}}
\bibcite{vannieuwenhoven2012new}{{90}{2012}{{Vannieuwenhoven et~al.}}{{Vannieuwenhoven, Vandebril, and Meerbergen}}}
\bibcite{vasilescu2002multilinear}{{91}{2002}{{Vasilescu and Terzopoulos}}{{}}}
\bibcite{wang2012semi}{{92}{2012}{{Wang et~al.}}{{Wang, Kumar, and Chang}}}
\bibcite{wang2015fast}{{93}{2015}{{Wang et~al.}}{{Wang, Tung, Smola, and Anandkumar}}}
\bibcite{wong2017lasso}{{94}{2017}{{Wong and Tewari}}{{}}}
\bibcite{woodruff2014sketching}{{95}{2014}{{Woodruff et~al.}}{{}}}
\bibcite{woolfe2008fast}{{96}{2008}{{Woolfe et~al.}}{{Woolfe, Liberty, Rokhlin, and Tygert}}}
\bibcite{wright2009robust}{{97}{2009}{{Wright et~al.}}{{Wright, Yang, Ganesh, Sastry, and Ma}}}
\bibcite{wu2015uniform}{{98}{2015}{{Wu and Zaffaroni}}{{}}}
\bibcite{yurtsever2017sketchy}{{99}{2017}{{Yurtsever et~al.}}{{Yurtsever, Udell, Tropp, and Cevher}}}
\bibcite{zhou2014decomposition}{{100}{2014}{{Zhou et~al.}}{{Zhou, Cichocki, and Xie}}}
\bibcite{zuo2010growing}{{101}{2010}{{Zuo et~al.}}{{Zuo, Kelly, Di~Martino, Mennes, Margulies, Bangaru, Grzadzinski, Evans, Zang, Castellanos, and Milham}}}
\bibcite{zygmund2002trigonometric}{{102}{2002}{{Zygmund}}{{}}}
