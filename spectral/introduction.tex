\section{Introduction}\label{introduction}

Multivariate spectral density estimation is an important problem in time series and signal processing, with applications in many scientific disciplines including economics \citep{granger1969investigating}  and neuroscience \citep{bowyer2016coherence}. Spectral density of a stationary multivariate time series is the frequency domain analogue of covariance and is based on the Fourier transform of autocovariance function. It aggregates information on linear association, both contemporaneous and across different lags,  among the components of a multivariate time series. So it can be used to provide a richer description of cross-sectional dependency than Pearson correlation, which only accounts for contemporaneous association among the time series components. 

In particular, multivariate spectral density and coherence (frequency domain analogue of correlation) are routinely used in neuroscience as metrics of  functional connectivity among brain regions using time series of neurophysiological signals (e.g., fMRI, EEG and MEG) and to construct networks of interactions in a data-driven fashion \citep{bowyer2016coherence}. These connectivity networks, where each node corresponds to a brain region and edge weights correspond to strengths of coherence between regions, are often used to study differential brain connectivity patterns in patients suffering from neurological disorders. More recently, coherence metrics have also been used to construct similarity measures when clustering high-dimensional time series of brain signals \citep{euan2016hierarchical}. With advances in data collection and storage technologies, it is now feasible to analyze time series data on a large number of brain regions. For instance, the freeSurfer brain atlas used in this paper summarizes voxel level data to $p = 86$ brain regions. Consequently, there is an increasing interest among neuroscientists in constructing coherence networks among a large number of brain regions in a principled manner from temporally dependent samples of small to moderate size ($n \ll p^2)$. For instance, we use only $n=200$  samples for our fMRI data analysis in this paper.

This recent interest in learning the cross-sectional dependence from spectral density matrix at different frequencies is complementary to developments in classical time series and signal processing literature, which focused more on studying the \textit{shape} of spectral density function in a low-dimensional asymptotic regime ($p$ fixed, $n \rightarrow \infty$) \citep{brillinger1981time, brockwell2013time}.  In another line of work, \cite{dahlhaus1997identification, dahlhaus2003causality, eichler2007frequency} investigated in depth the issues of inference with coherence and testing of marginal independence between components of multivariate time series using integrated spectral density. Finer and uniform convergence rates of smoothed periodograms were more recently provided by \cite{wu2015uniform}. However, as the dimension of the time series increases, so does the estimation risk of smoothed periodograms. This was first pointed out by \cite{bohm2009shrinkage}, who showed that shrinking smoothed periodogram towards a simpler structure can reduce risk and make the estimates better-conditioned for studying inverse spectral density matrix. 
The authors also proved consistency of their estimates under a double-asymptotic regime $p \rightarrow \infty, n \rightarrow \infty, p^2/n \rightarrow 0$. In a series of papers, \citet{bohm2008structural,  fiecas2016dynamic,  fiecas2014datadriven} have made significant progress in this direction by providing a wide variety of shrinkage methods with attractive theoretical and empirical properties.

In this work, we make two additions  to this research direction of learning large spetral density matrices. First, we propose a family of \textit{sparsity regularized estimators} of spectral density matrix based on thresholding averaged periodograms. Our proposed estimators have the added advantage of performing  automatic edge selection and providing sparse, interpretable networks among the component time series. Second, we develop a non-asymptotic theory for estimation of  spectral density and coherence that explicitly connects estimation error bounds to a notion of approximate sparsity of the true spectrum. As a consequence, our theory shows that consistent estimation is possible in a high-dimensional regime $\log p / n \rightarrow 0$ as long as the underlying structure is approximately sparse. 

Our proposal is motivated by recent  developments in covariance matrix estimation literature, where several thresholding based strategies \citep{bickel2008covariance, rothman2009generalized, cai2011adaptive, cai2016rates} have shown to provide good theoretical and empirical properties compared to the shrinkage based estimators proposed in \citet{ledoit2004well}. The thresholding techniques developed in this literature serve as promising candidates for high-dimensional spectral density estimation as well. However, their implementation and theoretical analysis require addressing additional technical challenges. From an implementation consideration, choice of threshold in covariance matrix estimation for i.i.d. data is carried out using multiple sample-splitting \citep{bickel2008covariance} which is not feasible when the data have a temporal ordering. On the theoretical side, non-asymptotic analysis of  periodograms averaged across nearby frequencies requires understanding concentration behavior of a sum of random matrices that are \textit{neither independent nor identically distributed}. Unlike sample covariance estimation with i.i.d. data, the lack of identical distribution results in smoothing bias well-known in nonparametric density estimation. In addition, the additional temporal dependence complicates deriving finite sample deviation of averaged periodogram from its expectation. %Finally, analyzing finite sample concentration of averaged periodogram about its expectation require developing deviation bounds for quadratic forms of complex random variables currently not available in the literature. 

We make three technical contributions in this paper to address the above challenges. First, we select thresholding parameters using a frequency-domain sample-splitting scheme based on the heuristic of approximate independence of periodograms at different Fourier frequencies. Second, we provide upper bounds on the finite sample bias of averaged periodograms and provide insight into how it is affected by temporal dependence in data for some commonly used families of time series. Finally, we develop a non-asymptotic upper bound on the deviation of averaged periodogram using a Hanson-Wright type inequality for complex quadratic forms of temporally dependent random vectors. Building upon these technical ingredients, our main theoretical results include (i) consistency of thresholded averaged periodograms in operator and scaled Frobenius norms in a high-dimensional regime under a weak sparsity assumption on true spectrum, and (ii)  sparsistency results ensuring selection of marginally correlated pairs of time series in a coherence network with high probability. Our analysis  framework accommodates Gaussian time series, and linear processes with subGaussian or generalized subexponential errors, or errors with finite fourth moments. The rates of convergence of thresholded estimators change with the nature of tail distribution of errors.

We demonstrate the merits of our proposed methods using extensive numerical experiments and a real data application on constructing functional connectivity networks from fMRI data. Our numerical experiments show that thresholding methods achieve  estimation accuracy comparable with  the shrinkage method, while simultaneously performing automatic coherence selection. In particular, a  lasso and an adaptive lasso based thresholding strategy show promising performance across different simulation settings. In the real data application, these two methods were able to extract  sparse, interpretable networks that nicely captured known biological patterns in brain networks and distinguished different brain regions from each other.





The rest of the paper is organized as follows. In section \ref{sec:model-methods}, we formally introduce our problem, provide a brief review of shrinkage estimators, and describe our proposed thresholding methods. In section \ref{sec:theory}, we derive non-asymptotic upper bounds on our proposed spectral density estimates for Gaussian time series. In section \ref{sec:heavy-tail} we extend the results for Gaussian time series to general linear processes with different non-Gaussian noise distributions. In section \ref{sec:simulation}, we conduct simulation studies to assess the finite sample properties of our proposed estimators. Section \ref{sec:realdata} contains an empirical application of our proposed method to a functional connectivity analysis with real fMRI data. We defer the proofs of all of our technical results to the Appendix. 



\textbf{Notation.} Throughout this paper, $\mathbb{Z}$, $\mathbb{R}$ and $\mathbb{C}$  denote the sets of integers, real numbers and complex numbers, respectively. We use $|c|$ to denote the modulus of a complex number and the absolute value of a real number. We use $\|v\|$ to denote $\ell_2$-norm of a vector $v$. For a matrix $A$, $\|A\|_1$, $\|A\|_{\infty}$, $\|A\|$ and $\|A\|_F$ will denote maximum complex modulus column sum norm, maximum complex modulus row sum norm, { spectral norm} $\sqrt{\Lambda_{\max}(A^\dag A)}$ and Frobenius norm $\sqrt{\text{tr}(A^\dag A)}$, respectively, where $A^\dag$ is conjugate transpose of $A$. We also let $\lambda_{\text{max}}(A)$ denote the spectral radius of a $n \times n$ matrix $A$, i.e., $\lambda_{\text{max}}(A) = \max(|\lambda_1|, \cdots |\lambda_n|)$, where $\lambda_i$ are the eigenvalues of matrix $A$. If $A$ is symmetric or Hermitian, we denote its maximum and minimum eigenvalues by $\Lambda_{\min}(A)$ and $\Lambda_{\max}(A)$. We use $e_i$ to denote the $i^{th}$ unit vector in $\mathbb{R}^p$, for $i = 1, 2, \ldots, p$. For vectors $v_i \in \mathbb{R}^p, i=1,\ldots, n$, we use $[v_1:\ldots:v_n]$ to denote the $p \times n$ matrix formed  by horizontally stacking these column vectors $v_i$, and  $[v_1^\top;\ldots; v_n^\top]$ to denote the $n\times p$ matrix by vertically stacking row vectors $v_i^\top$. Let $vec(A)$ represent the vector got from vectorization of a matrix $A$ by stacking all its columns. We use $rk(A)$ to denote the rank of a matrix $A$. For a complex vector $v\in \mathbb{C}^p$ and any $q > 0$, we define $\|v\|_q:= (\sum_{i=1}^p |v_i|^q)^{1/q}$. We use $\|v\|_0$ to denote the number of non-zero elements in $v$. Note that when $0\le q<1$, it is not really a norm since triangle inequality does not hold, but we keep the notation of a norm for convenience . Then we define the induced matrix norm, $\|A\|_{\alpha, \beta} = \sup_{x\neq 0}\|Ax\|_\alpha/\|x\|_\beta$, for any  $\alpha>0, \beta>0$. We will also use $\|A\|_\alpha$ to denote the induced norm $\|A\|_{\alpha, \alpha}$ for any $\alpha > 0$ and any complex matrix $A \in \mathbb{C}^{p \times p}$. Also, to be succinct, we use $\|A\|_{\rm{max}} :=\max_{r,s}|A_{rs}|$. 
Throughout the paper, we write $A \succsim B$ if there exists a  universal constant $c > 0$, not depending on model dimension or any model parameters, such that $A \ge cB$. We use $A \asymp B$ to denote $A \succsim B$ and $B \succsim A$. 
% In spectral density estimation, $\Omega_n(f_X)$ and $L_n(f_X)$ will measure the estimation error of autocovariance which will be discussed in details later. 

\iffalse
{\color{red} [Note: motivate weak sparsity; highlight contribution of new tuning parameter selection method in freq domain; see if bringing description of shrinkage methods in here makes sense; after simulation, add discussion of heterogeneity in addition to weak sparsity; do careful literature search about other competing shrinkage methods; add success stories of coherence metrics in neuroscience].}
\begin{enumerate}
    \item propose (soft/hard)-thresholding based spectral density estimates which comes with automatic variable selection (rather than shrinkage);
    \item non-asymptotic theoretical analysis that holds in high-dimensional regime $\log p /n \rightarrow 0$ under weak sparsity; results hold for linear processes with non-Gaussian errors; general purpose concentration bounds involving DFT, useful in other contexts as well (approximate independence of DFT's generalize to high-dim setting, also joint guaranteee across m frequencies -- check Wei Biao's paper for other usage of such a result); require finite sample analysis of bias since tuning parameter choice depends on it;
    \item novel method of tuning parameter selection;
    \item numerical evidence supporting the merit of thresholding approach for heterogeneous structures;
    \item Application to real EEG data [findings].
\end{enumerate}
\fi









